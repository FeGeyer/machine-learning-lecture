{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A word on what machines can (and cannot) do\n",
    "\n",
    "The term *Artifical Inteligence* was coined by John McArthy for the 1956 Dartmouth Conference. \n",
    "\n",
    "The idea of a *mechanical brain* is much older. \n",
    "\n",
    "<img width=\"45%\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/8b/Tuerkischer_schachspieler_windisch4.jpg\" />   \n",
    "\n",
    "\n",
    "The Church-Turing Thesis\n",
    "\n",
    "> Every effectively calculable function is a computable function.\n",
    "\n",
    "Using Turings definitions of computability\n",
    "\n",
    "> \"We shall use the expression 'computable function' to mean a function calculable by a machine, and let 'effectively calculable' refer to the intuitive idea without particular identification with any one of these definitions.\"\n",
    "\n",
    "To put it into somewhat more understandable words:\n",
    "\n",
    "* If something is intuitivly calculable (in whatever manner you can think of), it can be computed by a machine. \n",
    "\n",
    "* If something can be computed by a machine, it can be computed by a brain. (given enough time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<div class=\"bottom\">\n",
    "  <div style=\"float: left; width: 70%; padding-right:5em;\">\n",
    "  \n",
    "  __Are computers just as good as human brains then?__\n",
    "\n",
    "\n",
    "<p>They seem to work well together.</p>\n",
    "\n",
    "<p>Computer assisted proof of the Four-Color-Problem in 1989: </p>\n",
    "\n",
    "<blockquote>  ...part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand with the assistance of Haken's daughter... </blockquote>\n",
    "\n",
    "\n",
    "Easy problem for machines:\n",
    "<ul>\n",
    "<li>  Is the text in the image written in the english language?</li>\n",
    "</ul>\n",
    "Impossible problem for machines:\n",
    "<ul>\n",
    "<li> What does this text mean?</li>\n",
    "</ul>\n",
    "\n",
    "A few things to keep in mind while talking about artifical intelligence\n",
    "<ul>\n",
    "    <li> \n",
    "    Logical reasoning (within some limitations) is possible for machines as long as all relations and symbols are strictly defined \n",
    "    </li>\n",
    "    <li> \n",
    "    Mapping syntax and symbols to semantics and objects in the real world is something inherently human (or intelligent).\n",
    "    </li>\n",
    "    <li> \n",
    "    There is not really a powerfull formal equivalent for human creativity.\n",
    "    </li>\n",
    "    <li> \n",
    "    Machines cannot \"learn\"\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "  </div>\n",
    "  <div style=\"float: left; width: 30%;\">\n",
    "      <img width=\"100%\" src=\"./ml/images/alice.jpg\"/>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><br><br><br><br><br><br><br><br><br><br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning (Supervised)\n",
    "\n",
    "## Mathematical Notation and Problem Description\n",
    "\n",
    "*For more details see \"Elements of statistical Learning\" by Trevor Hastie. (Available for free as an E-Book)* \n",
    "\n",
    "I'll try to follow some naming conventions along this notebook. They are the same as in the book (for the most parts).\n",
    "\n",
    "* Uppercase letters such as $X$ or $Y$ denote generic aspects of a variable (i.e. the actual random variable)\n",
    "* Observed values are written in lowercase. The ith observed value of $X$ is written as $x_i$\n",
    "* Matrices are written in bold uppercase letters as in $\\mathbf{X}$\n",
    "* Observations map as *rows* in the matrix while the observed variables are the *columns*.\n",
    "\n",
    "So if I measure two observables $p = 2$ the size and weight of $N = 100$ people, I get a $N \\times p$ matrix $\\mathbf{X}$.\n",
    "One observation in that matrix is denoted as $x_i = [ size, weight ]$ while all observations of the variable size are denoted by $\\mathbf{x}_j$ \n",
    "\n",
    "Heres one possible definition of supervised machine learning:\n",
    "\n",
    "> Given a $N \\times p$ matrix $\\mathbf{X}$ and some associated output vector $\\mathbf{Y} \\in \\mathbb{R}^N$,\n",
    " find a function $f(X) = \\hat{Y}$ that takes a vector $X \\in \\mathbb{R}^p$ and returns a prediction for $\\hat{Y}$\n",
    " where some \"loss function\" $L(Y, f(X))$ is minimized for all $X$.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The Titanic Example. Learning from disaster.\n",
    "\n",
    "In the spring of 1912 the R.M.S. Titanic embarked on a journey to cross the Atlantic ocean. Unfortunately it hit an iceberg on the night of April 14th and sank shortly afterwards.\n",
    "\n",
    "The disaster caused widespread outrage over what was seen as lax safety regulations and reckles behavoiur by some. New maritime safety laws were put in place after the sinking that are still in place today.\n",
    "\n",
    "What can _we_ learn from the Titanic just by looking at its passenger data?\n",
    "\n",
    "Our data contains a list of name, gender, age and ticket price for each (known) passenger.  \n",
    "\n",
    "![NYT headline about the Titanic](./ml/images/nyt_titanic.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ml import plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "discrete_cmap = LinearSegmentedColormap.from_list('discrete', colors = [(0.8, 0.2, 0.3), (0.98, 0.6, 0.02), (0.1, 0.8, 0.3), (0, 0.4, 0.8), ], N=4)\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "plots.set_plot_style()\n",
    "def read_titanic():\n",
    "    data = pd.read_csv('./titanic_train.csv', index_col='PassengerId').dropna(subset=['Age'])\n",
    "    data['Survived_Code'] = data.Survived\n",
    "    data['Pclass_Code'] = data.Pclass\n",
    "    data.Survived = pd.Categorical.from_codes(data.Survived, categories=['no', 'yes'])\n",
    "    data.Pclass = pd.Categorical.from_codes(data.Pclass - 1, categories=['1st', '2nd', '3rd'])\n",
    "    data.Sex = pd.Categorical(data.Sex)\n",
    "    data['Sex_Code'] = data.Sex.cat.codes\n",
    "    return data\n",
    "\n",
    "data = read_titanic()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.Survived.value_counts().plot.pie(autopct='%.2f %%', cmap=discrete_cmap)\n",
    "plt.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__The task:__\n",
    "\n",
    "Given a vector $X = (Name, Class, Age, Sex)$ can we find a function $f_{survival}(x) \\in \\{{yes, no}\\}$ that accurately predicts the survival of the passengers in most cases?\n",
    "\n",
    "How do we know if that function $f_{survival}(x)$  is any good?\n",
    "\n",
    "To get some sense of the quality of this predictor we gather the following numbers.\n",
    "\n",
    "* __True Positives__ $TP$, The number of correctly predicted events that belong to the 'positive' class\n",
    "* __False Positives__ $FP$, The number of events falsely predicted as positive that actually belong to the 'negative' class\n",
    "* __True Negatives__ $TN$, The number of correctly predicted events that belong to the 'negative' class\n",
    "* __False Negatives__ $FN$, The number of events falsely predicted as negative that actually belong to the 'positive' class\n",
    "\n",
    "\n",
    "We can look at the fraction of correctly labeled observations in the data\n",
    "\n",
    "$$\n",
    "    accuracy(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{N} \\sum_{i = 1}^N \\mathbb{1}(y_i = \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "or simply put \n",
    "\n",
    "$$\n",
    "    accuracy(\\mathbf{y}, \\mathbf{\\hat{y}}) =  \\frac{TP + TN}{ TP + FP + FN + TN} = \\frac{\\text{correclty predicted}}{\\text{total number of observations}}.\n",
    "$$\n",
    "\n",
    "\n",
    "Now we try to find a function where the accuracy is higher than 0.5\n",
    "\n",
    "\n",
    "__One possible solution__:\n",
    "\n",
    "Women and children first?\n",
    " \n",
    "```\n",
    "def f_survival(passenger):\n",
    "    if passenger.Sex == 'female':\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def f_sex(passenger_sex):\n",
    "    return 'yes' if passenger_sex == 'female' else 'no'\n",
    "\n",
    "data = read_titanic()\n",
    "truth = data['Survived']\n",
    "prediction = data['Sex'].apply(f_sex)\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=truth, prediction=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we improve the classification? \n",
    "\n",
    "We could try yo learn more about what happened on the Titanic. Essentially using *expert* knowledge.\n",
    "\n",
    "Perhaps even by watching that 1997 movie where Leonardo Di Caprio drowns in the end. \n",
    "\n",
    "In the movie Di Caprios character dies along with many of his third class passenger friends.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<img width=\"50%\" src=\"./ml/images/titanic-movie.jpg\"/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "Let's presume rich people get to go into lifeboats.\n",
    "\n",
    "```\n",
    "def f_class(passenger):\n",
    "    if passenger.class == '1st':\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f_class(passenger_class):\n",
    "    return 'yes' if passenger_class == '1st' else 'no'\n",
    "\n",
    "data = read_titanic()\n",
    "prediction = data['Pclass'].apply(f_class)\n",
    "truth = data['Survived']\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=truth, prediction=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Models\n",
    "\n",
    "Can we improve our predictor by combining more variables into one predictor?\n",
    "\n",
    "Lets presume a linear weighted combination of variables:\n",
    "\n",
    "$$\n",
    "f(x)= \\hat{y} =  \\hat{\\beta}_0 + \\sum_{j=1}^p x_j \\hat{\\beta}_j\n",
    "$$\n",
    "where $f:\\mathbb{R}^{p} \\to \\mathbb{R}$.\n",
    "\n",
    "For a single sample of the Titanic data we simply evaluate:\n",
    "$$\n",
    "f(x)= \\hat{\\beta}_0 + x_{Class} \\hat{\\beta}_1 + x_{Sex} \\hat{\\beta}_2\n",
    "$$\n",
    "\n",
    "When we include a 1 as the first entry into our sample $x$ e.g. $x = (1, x_1, x_2, \\ldots, x_p)$ we can rewrite\n",
    "$f$ in matrix form\n",
    "\n",
    "$$\n",
    "f(x)= \\hat{y} =  x^T \\mathbf{\\beta}\n",
    "$$\n",
    "\n",
    "where $\\beta = (1, \\beta_1, \\beta_2, \\ldots, \\beta_p)$.\n",
    "\n",
    "\n",
    "\n",
    "How do you find those weights? Like before we choose a loss function and try to opimize it.\n",
    "In this case we choose a loss function called the residual sum of squares (RSS).\n",
    "We calculate it over all samples $x_i$ in a matrix $\\mathbf{X}$.\n",
    "\n",
    "$$L(\\beta) = RSS(\\mathbf{\\beta}) = \\sum_{i=1}^N (y_i - x_i^T \\beta)^2 $$\n",
    "\n",
    "Here $x_i$ is a row in $\\mathbf{X}$, hence the transpose.\n",
    "\n",
    "We can now rewrite the loss function in matrix form:\n",
    "\n",
    "\n",
    "$$\n",
    "RSS(\\beta) = (\\mathbf{y} - \\mathbf{X} \\beta)^T (\\mathbf{y} - \\mathbf{X} \\beta )\n",
    "$$\n",
    "\n",
    "Now we optimize the loss function just like we would any other function, by differentiating with respect to $\\beta$ and setting the result equals to zero.\n",
    "\n",
    "$$\n",
    " \\mathbf{X}^T (\\mathbf{y} - \\mathbf{X} \\beta ) \\stackrel{!}{=} 0\n",
    "$$\n",
    "\n",
    "Solving for $\\beta$ leads to\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "We just performed  __Linear Least Squares__ regression.\n",
    "\n",
    "Now we can define a function to predict passenger survival according to\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}\n",
    "\\text{Yes}, & \\text{if $ f(x) \\gt 0.5$} \\\\\n",
    "\\text{No}, & \\text{if $ f(x) \\le 0.5$}\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Linear Regression and Classification with sklearn\n",
    "\n",
    "Create an artificial 2D dataset with two classes and use the least squares method to seperate them.\n",
    "\n",
    "\n",
    "1. Create random points in a 2D parameter space\n",
    "        \n",
    "        from sklearn.datasets import make_blobs\n",
    "        \n",
    "2. Use scikit-learn's linear regressor to find the parameters for $f(X_1, X_2) = \\hat{Y}$.\n",
    "\n",
    "        from sklearn import linear_model\n",
    "        reg = linear_model.LinearRegression()\n",
    "        reg.fit(X, Y)\n",
    "        b_1, b_2 = reg.coef_\n",
    "        b_0 = reg.intercept_\n",
    "\n",
    "3. Draw a dashed line into the plot where $f(X_1, X_2) = 0.5$.\n",
    "\n",
    "        x1s = np.linspace(-2, 2)\n",
    "        x2s = ...\n",
    "\n",
    "        plt.plot(x1s, x2s, color='gray', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=400, centers=2, cluster_std=0.5, random_state=0)\n",
    "\n",
    "# train the linear regressor and save the coefficents\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "b_1, b_2 = reg.coef_\n",
    "b_0 = reg.intercept_\n",
    "\n",
    "# solve the function y = b_0 + b_1*X_1 + b_2 * X_2 for X2\n",
    "x1s = np.linspace(-1, 4)\n",
    "x2s = (0.5 - b_0 - b_1 * x1s) / b_2\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=25, c=y, cmap=discrete_cmap)\n",
    "\n",
    "\n",
    "plt.plot(x1s, x2s, color='gray', linestyle='--')\n",
    "\n",
    "plt.fill_between(x1s, x2s, 6, color='red', alpha=0.06)\n",
    "plt.fill_between(x1s, x2s, -1, color='blue', alpha=0.06)\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.margins(x=0, y=0)\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We just *learned* the parameters for a statistical model based on labeled data.\n",
    "\n",
    "Can a linear classification improve the classification of the Titanic dataset case?\n",
    "\n",
    "We have to evaluate our 'learned' model independent test set\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=0)\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_prediction = reg.predict(X_test)\n",
    "y_prediction = np.where(y_prediction > 0.5, 1, 0)\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model does not seem to improve the classification to a large degree. \n",
    "\n",
    "\n",
    "We will talk more about properly validating models later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(data.Fare, data.Age, data.Sex_Code, c=data.Survived_Code, cmap=discrete_cmap)\n",
    "ax.set_xlabel('Fare / $')\n",
    "ax.set_ylabel('Age / Years')\n",
    "ax.set_zlabel('Sex')\n",
    "ax.set_zticks([0,1])\n",
    "ax.set_zticklabels(['Male', 'Female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Different classification algorithms optimize different loss functions. In the previous section we optimized the residual sum of squares. Now we will be looking at the __0-1 loss__.\n",
    "\n",
    "\n",
    "$$ L_{0,1}(\\hat{Y}, Y) = 1 - \\delta(Y, \\hat{Y}) =    \\begin{cases}\n",
    "                                                    1 & {\\text{if}}\\quad Y \\neq \\hat{Y} \\\\\n",
    "                                                    0 & {\\text{else}} \n",
    "                                                \\end{cases}\n",
    "$$\n",
    "\n",
    "This loss is used for classification problems as it basically counts the number of misclassifications and works best on discrete values of $Y$.\n",
    "\n",
    "One possible Idea is to try and minimize the 0-1 loss directly.\n",
    "\n",
    "Presume we knew $P(Y| X)$ i.e the probability for the value of the label $Y$ in presence of a data point.\n",
    "Then intuitively the best classifier $f(x) = \\hat{y}$ is the one that minimizes the conditional expected loss.\n",
    "\n",
    "\\begin{align}\n",
    "    E(L_{0,1}(\\hat{y}, y) | x) =& \\sum_{y \\in Y}P(y| x) L_{0,1}(\\hat{y}, y) \\\\\n",
    "            =& \\sum_{y \\in Y}  P(y| x) (1 - \\delta(\\hat{y}, y)) \\\\\n",
    "            =& \\sum_{y \\in Y}  P(y| x) - \\sum_{y \\in Y} \\delta(\\hat{y}, y)) P(y| x) \\\\\n",
    "            =& 1 - \\sum_{y \\in Y} \\delta(f(x), y)) P(y| x) \\\\\n",
    "            =& 1 - P(\\hat{y}| x) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now if we minimze the expected loss given a sample $x$ while varying the predictor function $f(x) = \\hat{y}$ we get   \n",
    "<p style=\"color:gray\"> See <a href=\"https://de.wikipedia.org/wiki/Arg_max\" style=\"color:gray\">https://de.wikipedia.org/wiki/Arg_max</a> </p>\n",
    "\n",
    "\\begin{align}\n",
    "f_{*} =& \\arg \\min_f(E(L_{0,1}(f(x), y) | x)) = \\arg \\min_{\\hat{y}}(E(L_{0,1}(\\hat{y}, y) | x)) \\\\\n",
    "    =& \\arg \\min_{\\hat{y}} \\left(  1 - P(\\hat{y}| x) \\right) \\\\\n",
    "    =& \\arg \\max_{\\hat{y}} \\left( P(\\hat{y}| x) \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "This is a special case of the so called the maximum a posteriori principle (MAP https://de.wikipedia.org/wiki/Maximum_a_posteriori) which can be used to estimate the mode of the posterior distribution.\n",
    "\n",
    "\n",
    "Keep in mind that $x$ is a vector of $p$ observables. \n",
    "\n",
    "$$\n",
    "P(\\hat{y} | x^{(1)}, \\ldots, x^{(p)}) \n",
    "$$\n",
    "\n",
    "Using Bayes' theorem (https://en.wikipedia.org/wiki/Bayes'_theorem) for conditional probabilities we get \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "f_{*} =& \\arg \\max_{\\hat{y}}\\left( P(\\hat{y} |x^{(1)}, \\ldots, x^{(p)})  \\right) \\\\\n",
    " =& \\arg \\max_{\\hat{y}}\\left(\\frac{P(\\hat{y}) \\cdot P(x^{(1)}, \\ldots, x^{(p)} | \\hat{y})} {P(x^{(1)}, \\ldots, x^{(p)})} \\right) \\\\\n",
    "  =& \\arg \\max_{\\hat{y}}\\left(P(\\hat{y}) \\cdot P(x^{(1)}, \\ldots, x^{(p)} | \\hat{y}) \\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "The problem is that usually the a priori distributions (aka the prior) is unknown and has to be approximated using strong assumptions.\n",
    "\n",
    "The __Naive Bayes__ classifier does exactly that. Let's assume that the features are completely independent of each other. In that case we can write \n",
    "\n",
    "$$\n",
    "P(x^{(1)}, \\ldots, x^{(p)} | \\hat{y}) = \\prod^{p}_{i=1} P(x^{(i)}| \\hat{y})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "f_{*} = \\arg \\max_{\\hat{y}}\\left( P(\\hat{y}) \\prod^{p}_{i=1} P(x^{(i)}| \\hat{y})  \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "When $Y$ takes finite values (or classes) $Y = \\{Y_1, \\ldots Y_K\\}$, like in the Titanic dataset, we get the decission function for the Naive Bayes classifier \n",
    "\n",
    "$$\n",
    "y_{\\text{Naive Bayes}} = \\arg \\max_{k \\in {1, \\ldots, K}}\\left( P(Y_k) \\prod^{p}_{i=1} P(x^{(i)}| Y_k)  \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Note that the independency assumption is a very strong constraint. Features in real data are almost never independent of each other. __This assumption is almost always wrong__. (Hastie does call it the *idiots* classifier). \n",
    "\n",
    "Even under that assumption the probabilities $P(x^{(i)}| \\hat{y})$ are often not known. They can however often be approximated.\n",
    "\n",
    "In the Titanic dataset we simply assume uniform priors. That means $P(Y={\\text{Survived}}) = \\frac{\\text{number of passengers that survived}}{\\text{total number of passengers}}$.\n",
    "\n",
    "\n",
    "For the feature distributions/likelihoods we assume a Gaussian distribution. For a feature $\\mathbf{x_i}$, for example the age of a passenger, we assume a Gaussian distributions and empirically estimate both mean $\\mu_k$ and standard deviation $\\sigma_k$ of the samples of passengers in class $Y_k$. For a given value of an observable $v$, e.g. the age of a single passenger, we then compute \n",
    "\n",
    "$$\n",
    "p(x=v \\, | \\, Y_k) = \\mathcal{N}(\\mu_k, \\sigma_k)\n",
    "$$\n",
    "\n",
    "\n",
    "Below we perform Gaussian Naive Bayes on the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=0)\n",
    "\n",
    "reg = GaussianNB()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "The basic assumption underlying the least squares approach is that the model is linear in the observed variables. \n",
    "This works for data which can be separated by a linear function (a hyperplane in the parameter space).\n",
    "\n",
    "But how do we know that this method finds the 'best' hyperplane for separating the two classes?\n",
    "\n",
    "And what if the data cannot be seperated by a plane?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# many possible lines to separate the data. Which one is 'better'?\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=150, centers=2,\n",
    "                  random_state=3, cluster_std=0.70)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "\n",
    "xs = np.linspace(-6.5, 3, 2)\n",
    "plt.plot(xs, -2 * xs - 2, color='gray', linestyle='--')\n",
    "plt.plot(xs, -0.4 * xs + 2, color='gray', linestyle='--')\n",
    "plt.xlim([-6, 3])\n",
    "plt.ylim([-2, 6])\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Support Vector Machine\n",
    "\n",
    "Again we minimze a loss function.\n",
    "\n",
    "$$\n",
    "L(\\beta) = C \\max(0, 1 - y_i \\beta^T x_i) + \\frac{\\lambda}{2}||{\\beta}||^2\n",
    "$$\n",
    "\n",
    "Support Vector Machines try to find the hyperplane which maximimizes the margin to the points in different classes in the parameter space.\n",
    "\n",
    "$C$ and $\\lambda$ are two parameters which can be chosen beforehand. \n",
    "\n",
    "<p style=\"color:gray\"> Note that, to fit the definition above, the label encoding has to be $y_i \\in {-1, 1}$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=150, centers=2,\n",
    "                  random_state=3, cluster_std=0.70)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plots.draw_svm_decission_function(clf, colors='black', label='SVM')\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "plots.draw_linear_regression_function(reg, label='Linear Regression', color='gray', alpha=0.5)\n",
    "\n",
    "plt.xlim([-6, 3])\n",
    "plt.ylim([-2, 6])\n",
    "plt.legend(loc='lower right', frameon=True, framealpha=0.95, facecolor='white')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far the data has still been separable by a linear function. \n",
    "\n",
    "For many problems in real life however this isn't the case. \n",
    "\n",
    "Heres an example of (artificial) data which cannot be seperated by a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.10, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)\n",
    "plots.draw_svm_decission_function(clf, colors='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we take that data and transform it into a new variable. \n",
    "\n",
    "Find a function $h$ to create a new variable $X_h = h(X_1, X_2, \\ldots)$.\n",
    "\n",
    "In the case above some radial symmetry seems be an underlying feature of the data. \n",
    "\n",
    "We can exploit that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "X, y = make_moons(n_samples=400, noise=0.10, random_state=0)\n",
    "# add a dimension by applying a transformation on the two variables in the data. \n",
    "r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))\n",
    "\n",
    "#fig = plt.figure(figsize=(16, 6))\n",
    "#ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "plots.plot_3d_views(np.c_[X,r], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.10)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "\n",
    "clf = SVC(kernel='rbf') #use the radial basis function instead of the linear one.\n",
    "clf.fit(X, y)\n",
    "plots.draw_svm_decission_function(clf, colors='black', label='SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same approach works for other linear methods as well. \n",
    "\n",
    "What makes SVM's so special?:\n",
    "\n",
    "+ SVM's have proven to perform very well for many use-cases.\n",
    "\n",
    "+ SVM's handle large number of dimensions relativly fast.\n",
    "\n",
    "+ The kernel functions basically come for free.\n",
    "\n",
    "+ Easily extendable to multi-class problems.\n",
    "\n",
    "\n",
    "Kernel functions are constrained to fulfill certain criteria. *(See Chapter 12.3.1 in the Hastie Book)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "# Use linear kernel\n",
    "reg = SVC(kernel='linear')\n",
    "reg.fit(X_train, y_train)\n",
    "prediction_linear = reg.predict(X_test)\n",
    "\n",
    "# Use the rbf kernel\n",
    "reg_rbf = SVC(kernel='rbf')\n",
    "reg_rbf.fit(X_train, y_train)\n",
    "prediction_rbf = reg_rbf.predict(X_test)\n",
    "\n",
    "fig, ([ax1, ax2], [ax3, ax4]) = plt.subplots(2, 2, figsize=(12, 12))\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=prediction_linear, axes=[ax1, ax2], vmin=0, vmax=182)\n",
    "plots.plot_bars_and_confusion(truth=y_test, prediction=prediction_rbf, axes=[ax3, ax4], vmin=0, vmax=182)\n",
    "ax1.set_title('Linear Kernel')\n",
    "ax3.set_title('Radial Kernel')\n",
    "ax1.set_xlim([0, 300])\n",
    "ax3.set_xlim([0, 300])\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Extending SVMs to more dimensions\n",
    "\n",
    "The Titanic dataset we looked at in the previous excercise had four observed variables or dimensions and 714 observations in total. \n",
    "\n",
    "        > X = data[['Sex_Code', 'Pclass', 'Fare', 'Age']]\n",
    "        > X.shape\n",
    "        (714, 4)\n",
    "\n",
    "Now we take a dataset that has 64 variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "X, y = datasets.load_digits( return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "#SVC with default settings.\n",
    "clf = SVC(kernel='poly')\n",
    "\n",
    "# We learn the kernel on the first half of the data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Now predict the value of the digit on the test sample\n",
    "y_prediction = clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_prediction)\n",
    "\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='RdPu',\n",
    ")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What mystery data did we just classifiy?__\n",
    "\n",
    "The SVM can easily classifiy a dataset of many observables and target classes.\n",
    "\n",
    "This data set had 64 observables and 10 different classes.\n",
    "\n",
    "Lets take the 64 numbers in the single observations and plot them into a $8\\times8$ grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "plt.figure(figsize=(15, 1))\n",
    "plt.imshow([X[0]], aspect='auto', cmap='gray_r',)\n",
    "plt.grid('off')\n",
    "plt.yticks([])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(10, 10))\n",
    "for i, x_i in enumerate(X[:10]):\n",
    "    ax = axs[i]\n",
    "    img = x_i.reshape(-1, 8)\n",
    "    ax.imshow(img, cmap='gray_r', interpolation='nearest')\n",
    "    ax.grid('off')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This classifier just performed __character recognition__ on raw image inputs without any feature engineering.\n",
    "\n",
    "Currently there is a lot of buzz (or even hype) about image recognition tasks and neural networks (Deep Learning etc.)\n",
    "Neural networks and SVMs are very similar in nature they just use slightly modified loss functions.\n",
    "\n",
    "For further information please checkout Stanford's computer science lecture CS231n, especially the chapter on linear classification.\n",
    "\n",
    "[http://cs231n.github.io](http://cs231n.github.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Optimization and Decission Trees\n",
    "\n",
    "So far we looked at loss functions which optimized some global optimization criterion.\n",
    "\n",
    "We have seen that not all problems can be solved by a linear model. It is often not possible to\n",
    "find a transformation, or kernel function, to transform the data into linearly seperable classes.\n",
    "\n",
    "Neither by hand nor by some automated procedure. The idea of local optimization methods is to split the \n",
    "parameter space into subspaces where the problem is easier to solve.\n",
    "\n",
    "Idea:\n",
    "* Split the parameter space into many subspaces where observations of the same class live.\n",
    "\n",
    "We split the space into two regions \n",
    "\n",
    "$$\n",
    "R_1(j, s) = \\{X |\\, X_j \\le s\\}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "R_2(j, s) = \\{X |\\, X_j \\gt s\\}.\n",
    "$$\n",
    "\n",
    "These regions define half-planes in the parameter space.\n",
    "\n",
    "In the example below we define 4 half-planes/regions to select the region in space where the blue points life. \n",
    "Now we can optimize a loss function in each of the overlapping regions.\n",
    "\n",
    "In the example below we can simply use a decission function that is constant in each region.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^{M} c_m \\mathbb{I}(x \\in R_m)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathbb{I}(x \\in R_m) =\\begin{cases}\n",
    "1,  & \\text{if $x \\in R_m$} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, noise=0.10, random_state=0, factor=0.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "\n",
    "\n",
    "plt.axhspan(-0.5, 1.5, color='red', alpha=0.1)\n",
    "plt.axvspan(-0.5, 1.5, color='green', alpha=0.1)\n",
    "\n",
    "plt.axhspan(0.5, -1.5, color='blue', alpha=0.1)\n",
    "plt.axvspan(0.5, -1.5, color='yellow', alpha=0.1)\n",
    "\n",
    "plt.xlim([-1.4, 1.4])\n",
    "plt.ylim([-1.4, 1.4])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose the constant $c_m$ to be the average target value in that region. \n",
    "\n",
    "$$\n",
    "c_m  = \\frac{1}{N} \\sum_{x_i \\in R_m} y_i\n",
    "$$\n",
    "\n",
    "This works extremly well in the example below. The question remains on how to find the splits.\n",
    "This problem is again NP-Hard and therefore takes too long to solve. \n",
    "We can however employ a greedy strategy again. \n",
    "\n",
    "Algorithms performing these steps are called __Decission Tree__ methods.\n",
    "\n",
    "We perform recursive binary splits of the subspace using the split which yields the lowest value for the loss function. One advantage of binary splits is the straightforward intepretation of the results.\n",
    "The image below shows what a decission tree looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "np.random.seed(1234)\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.10)\n",
    "clf = DecisionTreeClassifier(max_depth=2, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(plots.draw_tree(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function this time is called the Information Gain. \n",
    "To find the best split in a node, we maximize the Information Gain, $IG$,  over all features  $X \\in \\mathbf{X}$ and splits $s$\n",
    "\n",
    "$$\n",
    "    IG(X,Y) = H(Y) - H(Y |\\, X).\n",
    "$$\n",
    "\n",
    "where $H$ is the entropy function\n",
    "\n",
    "\\begin{equation}\n",
    "  H(Y) = - \\sum_{z \\in Z} P(Y=z) \\log_2{P(Y=z)}.\n",
    "\\end{equation}\n",
    "\n",
    "for the random variable $Y$ which takes values from the finite set of symbols $Z$\n",
    "\n",
    "and $H(Y |\\, X)$ is the conditional entropy.\n",
    "\n",
    "\\begin{aligned}\n",
    "    H(Y |\\, X)  &=  \\sum_{m \\in M}P(X = m) H(Y |\\, X = m)  \\\\\n",
    "       &= - \\sum_{m \\in M}P(X = m) \\sum_{z \\in Z} P(Y = z |\\, X = m)\\log{P(Y = z |\\, X = m)}.\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "In the context of decission trees we want to know the entropy of $Y$ given some new information about $X$.\n",
    "In this context $H(Y)$ can be understood as the entropy of $Y$ before a split and $H(Y | \\, X)$ the entropy after the split. Probabilities are assumed to be uniform. Hence we get\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "  H(Y) =& - \\sum_{z \\in \\{\\text{Dead, Alive}\\}} P(Y=z) \\log_2{P(Y=z)} \\\\\n",
    "       =& -\\left( \\frac{ n_{ \\text{ dead }}}{n} \\log2{\\frac{ n_{ \\text{ dead }}}{n}} + \\frac{ n_{ \\text{ alive }}}{n} \\log2{\\frac{ n_{ \\text{ alive }}}{n}} \\right)\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "    H(Y |\\, X)  &=  \\sum_{g \\in \\{\\text{Male}, \\text{Female}\\} }P(X_{\\text{Sex}} = g) H(Y |\\, X = m)  \\\\\n",
    "        &=   \\frac{ n_{ \\text{male}}}{n} H(Y |\\, X_{\\text{Sex}} = \\text{male}) \\frac{ n_{ \\text{female}}}{n} H(Y |\\, X_{\\text{Sex}} = \\text{female}) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "where the specific conditioned entropy is \n",
    "\n",
    "\\begin{aligned}\n",
    "H(Y |\\, X_{\\text{Sex}} = \\text{male}) = -\\left( \\frac{ n_{ \\text{ dead }}}{n_{\\text{male}}} \\log2{\\frac{ n_{ \\text{ dead }}}{n_{\\text{male}}} }   +  \\frac{ n_{ \\text{ alive }}}{n_{\\text{male}}} \\log2{\\frac{ n_{ \\text{ alive }}}{n_{\\text{male}}} }     \\right)\n",
    "\\end{aligned}\n",
    "\n",
    "When using continuous features such as *Age* each possible split has to be tested. Using the nomenclature from above where $s$ is some split we can write down the total (local) loss function\n",
    "$$\n",
    "  \\max_{(X, s) \\in \\, \\mathbf{X} \\times {S}}IG(X,Y) =   \\max_{(X, s) \\in \\, \\mathbf{X} \\times {S}} ( H(Y) - H(Y |\\, X) ).\n",
    "$$\n",
    "\n",
    "We call this local optimization since we only evaluate the loss function in the subspace of the data defined by the decission tree. It is local to the current node of the decission tree. \n",
    "\n",
    "Here is another nice explanation of entropy and information gain\n",
    "https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain\n",
    "\n",
    "Outline for building a binary decission tree:\n",
    "\n",
    "        def build_tree(space)\n",
    "            if stopping_criterion_fulfilled():\n",
    "                return {}\n",
    "             \n",
    "            variable, split_point = find_best_split(space)\n",
    "                        \n",
    "            left, right = split_space(space, variable, split_point)\n",
    "\n",
    "            left_tree = build_tree(left)\n",
    "            right_tree = build_tree(right)\n",
    "               \n",
    "            return {'node' : (variable, split_point), 'left': left_tree, 'right': right_tree}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.10, random_state=0)\n",
    "clf = DecisionTreeClassifier(max_depth=2, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "plots.draw_decission_boundaries(clf, cmap=discrete_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we apply a decission tree to the Titanic dataset while varying some paramters like depth and which loss function to use. The results are plotted into a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "np.random.seed(1235)\n",
    "\n",
    "# load the data\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# loop over every possible combination of parameters \n",
    "df = pd.DataFrame()\n",
    "ps = ParameterGrid({'max_depth':range(1, 20), 'criterion':['entropy', 'gini']})\n",
    "for d in ps:\n",
    "    clf = DecisionTreeClassifier(max_depth=d['max_depth'], criterion=d['criterion'])\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    df = df.append({'max_depth': d['max_depth'], 'criterion': d['criterion'], 'accuracy': acc}, ignore_index=True)\n",
    "\n",
    "# plot a heatmap to see the best paramter combination. \n",
    "df = df.pivot('max_depth', 'criterion', 'accuracy')\n",
    "sns.heatmap(df, cmap='YlOrRd', annot=True, fmt='.3f', label='Accuracy')\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k - Nearest Neighbour Methods\n",
    "\n",
    "The k-NN classifier is anotherlocal optimizer.\n",
    "Lets assume that the decission function is constant over some local region in the parameter space:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x_0) = \\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x_o)} y_i\n",
    "$$\n",
    "\n",
    "where $x_i \\in N_k(x)$ describes the $k$ points in the training data $\\mathbf{X}$ that are in the *neighbourhood* of $x_0$.\n",
    "\n",
    "To put it in words. We assume $x$ will have the same $y$ as other points nearby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_moons\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=25)\n",
    "knn.fit(X, y)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works pretty well in this artifical, low-noise, example. \n",
    "\n",
    "Classification on noisy data will not work as good.\n",
    "\n",
    "Real world data always has some form of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.2)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=40)\n",
    "knn.fit(X, y)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax1)\n",
    "ax1.set_title('Accuracy for k=40 : {}'.format(accuracy_score(y, knn.predict(X))))\n",
    "ax1.axis('off')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax2)\n",
    "ax2.set_title('Accuracy for k=1 : {}'.format(accuracy_score(y, knn.predict(X))))\n",
    "ax2.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Validation \n",
    "\n",
    "So far we only looked at the *accuracy* measure of each classifier. We did not however compute any errors on that number. There are ways to compute errors on numbers like accuracy. First we try to motivate why these errors are needed.\n",
    "\n",
    "### Overfitting (Bias-Variance Tradeoff)\n",
    "\n",
    "Assume the target $y$ is generated by some function $f(x)$ with added gaussian noise $\\epsilon$\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon, \\qquad \\epsilon \\propto \\mathcal{N}(\\mu=0, \\sigma)\n",
    "$$\n",
    "\n",
    "The mean squared error ($mse$) of the predictor function $\\hat{f}(x) = \\hat{y}$ is\n",
    "\n",
    "$$\n",
    "mse(y, \\hat{f}) = (y - \\hat{f}(x))^2 .\n",
    "$$\n",
    "\n",
    "We can actually calculate the expectation value of the $mse$ for the k-NN classifier.\n",
    "\n",
    "$$\n",
    "E[mse(y, \\hat{f}) ] = E[(y - \\hat{f}(x))^2]\n",
    "$$\n",
    "\n",
    "Some mathematical definitions up front.\n",
    "\n",
    "__Variance__ of a random variable $X$ \n",
    "  \n",
    "  $$\n",
    "  Var(X) = E[(X - E[X])^2] = E[X^2] - E[X]^2 \\iff     E[X^2] = Var[X] +  E[X]^2 \n",
    "  $$\n",
    "  \n",
    "__Bias__ of an estimator $\\hat{f}$\n",
    "\n",
    "  $$\n",
    "  Bias(\\hat{f}) = E[\\hat{f} - f] = E[\\hat{f}] - E[f] = E[\\hat{f}] - f\n",
    "  $$\n",
    "\n",
    "The last equality holds since $f$ is a fixed, deterministic,  function and hence its values never change.\n",
    "\n",
    "$$\n",
    "E[f] = f\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Using the definition of the variance we can substitute our function $y = f(x) + \\epsilon$ gives \n",
    "\n",
    "\\begin{align}\n",
    "Var[y] = Var[ f(x) + \\epsilon]  & = Var[f(x)] + Var[\\epsilon] \\\\\n",
    "                                & = Var[f(x)] + \\sigma^2 \\\\\n",
    "                                & = E[f(x)^2] - E[f(x)]^2 + \\sigma^2 \\\\\n",
    "                                & = f(x)^2 - f(x)^2 + \\sigma^2 \\\\\n",
    "                                & = \\sigma^2 \n",
    "\\end{align}\n",
    "\n",
    "We can now simplify $E[mse(y, \\hat{f})]$:\n",
    "\n",
    "\\begin{align}\n",
    " E[(y - \\hat{f}(x))^2]  & = E[y^2 + \\hat{f}^2 - 2y\\hat{f}] \\\\\n",
    "                        & \\ldots \\\\\n",
    "                        & = \\sigma^2 + Var[\\hat{f}] + Bias[\\hat{f}]^2.\n",
    "\\end{align}\n",
    "\n",
    "For the k-NN classifier there exists an analytical expression for $E[mse(y, \\hat{f})]$.\n",
    "\n",
    "Using \n",
    "$$\n",
    "\\hat{f}(x) = \\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\n",
    "$$\n",
    "\n",
    "and once again assuming the data is subject to normally distributed noise $\\epsilon$\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\epsilon_i\n",
    "$$\n",
    "\n",
    "one can calculate \n",
    "\n",
    "$$\n",
    " E[(y - \\hat{f}(x))^2] = \\sigma^2 + Var[\\hat{f}] + Bias[\\hat{f}]^2.\n",
    "$$\n",
    "\n",
    "\n",
    "Starting with $Var(\\hat{f})$\n",
    "\n",
    "\\begin{align}\n",
    "Var(\\hat{f(x_0)}) &= Var \\left( \\frac{1}{k} \\sum_{y_i \\in N_k(x_0)} y_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2}  \\sum_{y_i \\in N_k(x_0)} Var \\left( f(x_i) + \\epsilon_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2}  \\sum_{y_i \\in N_k(x_0)} Var \\left( f(x_i) \\right) + Var \\left( \\epsilon_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2}  \\sum_{y_i \\in N_k(x_0)} Var \\left( \\epsilon_i \\right)  \\\\\n",
    "&= \\frac{1}{k^2} k \\sigma^2  \\\\\n",
    "&= \\frac{\\sigma^2}{k}\n",
    "\\end{align}\n",
    "\n",
    "Now the bias term\n",
    "\n",
    "\\begin{align}\n",
    "    Bias^2(\\hat{f}) & = E\\left[\\hat{f} - y \\right]^2 \\\\\n",
    "                    & = E\\left[\\frac{1}{k} \\left( \\sum_{y_i \\in N_k(x_0)} y_i \\right) - y \\right]^2 \\\\\n",
    "                    & = E\\left[\\frac{1}{k} \\left( \\sum_{x_i \\in N_k(x_0)} f(x_i) + \\epsilon_i \\right) - ( f(x_0) + e_0 )\\right]^2 \\\\\n",
    "                     & = E\\left[\\frac{1}{k} \\left( \\sum_{x_i \\in N_k(x_0)} f(x_i) \\right) - f(x_0) \\right]^2 \\\\\n",
    "                    & = \\left( \\frac{1}{k} \\left( \\sum_{x_i \\in N_k(x_0)} f(x_i) \\right) - f(x_0) \\right)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it together gives \n",
    "\n",
    "$$\n",
    " E_{knn}[(y - \\hat{f}(x))^2] = \\sigma^2 + Var[\\hat{f}] + Bias[\\hat{f}]^2 = \\sigma^2 + \\frac{\\sigma^2}{k} + \\left( \\frac{1}{k} \\left( \\sum_{y_i \\in N_k(x_0)} f(x_i) \\right) - f(x_0) \\right)^2.\n",
    "$$\n",
    "\n",
    "The expectation of mean squared error depends on the choice of $k$ and the noise of the data $\\sigma$. \n",
    "\n",
    "When $k$ grows, the dependency on the data noise decreases but the bias increases.\n",
    "\n",
    "This so called Bias-Variance dillemma is a universal problem in supervised machine learning. \n",
    "\n",
    "There are two error sources:\n",
    "\n",
    "* High bias might decrease overall predictor performance.\n",
    "* High variance can make the learned paramters prone to noise in the training data.  \n",
    "\n",
    "If the parameters are tuned to the noise in the training data, the model will not generalize to new data. \n",
    "\n",
    "This problem is called __overfitting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.7)\n",
    "X_test, y_test = make_moons(n_samples=100, noise=0.7)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax1)\n",
    "ax1.set_title('Accuracy on Training Data for k=1 : {}'.format(accuracy_score(y, knn.predict(X))))\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30, cmap='winter')\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y, s=15, alpha=0.1,  cmap='winter')\n",
    "plots.draw_decission_boundaries(knn, ax=ax2)\n",
    "ax2.set_title('Accuracy on Test Sample for k=1 : {}'.format(accuracy_score(y_test, knn.predict(X_test))))\n",
    "ax2.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(1234)\n",
    "X, y = make_moons(n_samples=400, noise=0.2)\n",
    "X_test, y_test = make_moons(n_samples=400, noise=0.2)\n",
    "\n",
    "e_train = []\n",
    "e_test = []\n",
    "for k in range(1, 200):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y)\n",
    "    e_train.append(mean_squared_error(y, knn.predict(X)))\n",
    "    e_test.append(mean_squared_error(y_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 200), e_train,'.', color='#FF6B6B', ms=10, label='Training Sample')\n",
    "plt.plot(range(1, 200), e_test, '.' ,color='#FFAE6B', ms=10, label='Test Sample', )\n",
    "plt.xlim(200, 0)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Prediction Error')\n",
    "\n",
    "plt.text(150, 0.026, 'Increasing Model Complexity')\n",
    "plt.arrow(150, 0.018, -50, 0, width = 0.0005, head_width=0.003, head_length=3, fc='k', ec='k')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Validation on independent test sets\n",
    "\n",
    "Validating the fitted models is essential for avoiding overfitting.\n",
    "\n",
    "The predictions error has to be assesed on an independent test dataset. \n",
    "\n",
    "Models might still be susceptible to noise in the training data.\n",
    "\n",
    "#### Cross Validation\n",
    "\n",
    "A $k$-fold cross validation automatically splits the training data into $k$ subsets.\n",
    "\n",
    "The model is then trained on $k-1$ subsets and evaluated on the remaining set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "    <style>\n",
    "    .training_set { fill: #FF6B6B;}\n",
    "    .test_set { fill: #FFAE6B; }\n",
    "    </style>\n",
    "<h4> 5-Fold Cross Validation </h4>\n",
    "<p> First Iteration: </p>\n",
    "<p> </p>\n",
    "<svg width=\"800\" height=\"140\">\n",
    "<g transform=\"scale(0.9)\">\n",
    "  <rect x=\"0\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"160\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"480\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"640\", y=\"0\", width=\"150\" height=\"80\" class=\"test_set\" />\n",
    "    \n",
    "  <rect x=\"0\", y=\"90\", width=\"630\" height=\"3\" class=\"training_set\" />\n",
    "  <rect x=\"640\", y=\"90\", width=\"150\" height=\"3\" class=\"test_set\" />\n",
    "    \n",
    "  <text x=\"0\" y=\"115\" class=\"training_set\">\n",
    "    Training Data\n",
    "  </text>\n",
    "  <text x=\"640\" y=\"115\" class=\"test_set\">\n",
    "    Test Data\n",
    "  </text>\n",
    "</g>\n",
    "</svg>\n",
    "\n",
    "<p> Second Iteration: </p>\n",
    "<p> </p>\n",
    "<svg width=\"800\" height=\"140\">\n",
    "<g transform=\"scale(0.9)\">\n",
    "  <rect x=\"0\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"160\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"480\", y=\"0\", width=\"150\" height=\"80\" class=\"test_set\" />\n",
    "  <rect x=\"640\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "    \n",
    "  <rect x=\"0\", y=\"90\", width=\"470\" height=\"3\" class=\"training_set\" />\n",
    "  <rect x=\"480\", y=\"90\", width=\"150\" height=\"3\" class=\"test_set\" />\n",
    "  <rect x=\"640\", y=\"90\", width=\"150\" height=\"3\" class=\"training_set\" />\n",
    "    \n",
    "  <text x=\"0\" y=\"115\" class=\"training_set\">\n",
    "    Training Data\n",
    "  </text>\n",
    "  <text x=\"480\" y=\"115\" class=\"test_set\">\n",
    "    Test Data\n",
    "  </text>\n",
    "</g>\n",
    "</svg>\n",
    "\n",
    "<p> Third Iteration: </p>\n",
    "<p> </p>\n",
    "<svg width=\"800\" height=\"140\">\n",
    "<g transform=\"scale(0.9)\">\n",
    "  <rect x=\"0\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"160\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"0\", width=\"150\" height=\"80\" class=\"test_set\" />\n",
    "  <rect x=\"480\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "  <rect x=\"640\", y=\"0\", width=\"150\" height=\"80\" class=\"training_set\" />\n",
    "    \n",
    "  <rect x=\"0\", y=\"90\", width=\"310\" height=\"3\" class=\"training_set\" />\n",
    "  <rect x=\"320\", y=\"90\", width=\"150\" height=\"3\" class=\"test_set\" />\n",
    "  <rect x=\"480\", y=\"90\", width=\"310\" height=\"3\" class=\"training_set\" />\n",
    "    \n",
    "  <text x=\"0\" y=\"115\" class=\"training_set\">\n",
    "    Training Data\n",
    "  </text>\n",
    "  <text x=\"320\" y=\"115\" class=\"test_set\">\n",
    "    Test Data\n",
    "  </text>\n",
    "</g>\n",
    "</svg>\n",
    "<p>...</p>\n",
    "<p>...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use sklearn's cross validation method to estimate the accuracy of a Decission Tree  classifier on many independent test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "r = cross_validate(clf, X=X, y=y, cv=5, scoring=['accuracy'])\n",
    "print('Accuracy on test sets: {:.3f} +/- {:.3f}'.format(r['test_accuracy'].mean(), r['test_accuracy'].std() ))\n",
    "print('Accuracy on training sets: {:.3f} +/- {:.3f}'.format(r['train_accuracy'].mean(), r['train_accuracy'].std() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Quality Metrics\n",
    "\n",
    "Accuracy is not always a good measure of model quality.\n",
    "\n",
    "Imagine a classifier function which simply predicts a fixed outcome.\n",
    "\n",
    "        def f_fixed(x):\n",
    "            return 0\n",
    "            \n",
    "On an imbalanced dataset this classifier will have an accuracy equal to the ratio of positive examples to the total number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fixed(x):\n",
    "    return np.zeros(shape=len(x))\n",
    "\n",
    "data = read_titanic()\n",
    "\n",
    "X = data\n",
    "y = data['Survived_Code']\n",
    "\n",
    "print('Accuracy of fixed classifier {:1.6f} \\n'.format(accuracy_score(y, f_fixed(X))))\n",
    "\n",
    "print('Ratio of survived to total passengers: ')\n",
    "print(data.Survived.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another view at the confusion matrix. It generalizes to more than two classes as seen in the picture below.\n",
    "\n",
    "![A confusion matrix](./ml/images/confusion_matrix.png)\n",
    "\n",
    "The numbers in the confusion matrix can be used to calculate a whole range of qulity criteria.\n",
    "\n",
    "Lets build a classifier which randomly chooses an outcome and look at the different criteria.\n",
    "\n",
    "```\n",
    "def f_random(passenger):\n",
    "    return np.random.choice(['yes', 'no')\n",
    "```\n",
    "\n",
    "\n",
    "<a href=\"https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\" style=\"color:#BBBBBB;\">The nice Stack Overflow post where I stole the picture from.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_random(x):\n",
    "    return np.random.randint(2, size=len(x))\n",
    "\n",
    "prediction = f_random(X)\n",
    "cm = confusion_matrix(y, prediction)\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "print(TPR, TNR, PPV, NPV, FPR, FNR, FDR, ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "print(recall_score(y, prediction))\n",
    "print(precision_score(y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precission and recall can be understood in an intuitive way\n",
    "\n",
    "* __Recall__ How many of the wanted examples are found.\n",
    "* __Precission__ The percentage of the found examples that are actually relevant.\n",
    "\n",
    "So what should you optimze for? Maximumg accuracy or precission or recall?\n",
    "\n",
    "There is no clear answer. It depends on your use-case. Can you tolerate false positives? \n",
    "Can you tolerate losing some true positives?\n",
    "\n",
    "We will always have to make the trade-off between recall and precission. \n",
    "There are several metrics which try to combine both into one.\n",
    "\n",
    "The $f_{\\beta}$ score is one example.\n",
    "\n",
    "$$\n",
    "f_{\\beta } = (1+\\beta ^{2})\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{(\\beta ^{2}\\cdot \\mathrm {precision} )+\\mathrm {recall} }} =\n",
    "\\frac {(1 + \\beta^2) \\cdot TP }{(1 + \\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}.\n",
    "$$\n",
    "\n",
    "But in the end there is no absolute truth to whats best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Discussion\n",
    "\n",
    "Imagine you devise a new, cheap and easy cancer test. \n",
    "What should you optimize your decission threshold for?\n",
    "Precission or recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "np.random.seed(1234)\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "results = []\n",
    "for clf, name in zip([svc, knn, tree], ['SVM', 'kNN', 'tree']):\n",
    "    r = cross_validate(clf, X=X, y=y, cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    df = pd.DataFrame().from_dict(r)\n",
    "    df['classifier'] = name\n",
    "    results.append(df)\n",
    "\n",
    "df = pd.concat(results).drop(['fit_time', 'score_time'], axis='columns')\n",
    "\n",
    "means = df.groupby('classifier').mean()\n",
    "deviations = df.groupby('classifier').std()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "sns.heatmap(means, cmap='viridis', annot=True, ax=ax1, vmin=0, vmax=1)\n",
    "sns.heatmap(deviations, cmap='viridis', annot=True, ax=ax2, vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decission Thresholds and Classifier Validation\n",
    "\n",
    "Classifier objects usually provide all of the following methods\n",
    "\n",
    "* `classifier.fit(X)` takes training data and finds some parameters based on that data.\n",
    "* `classifier.predict(X_new)` takes new data (one row or many) and predicts the target label for each row.\n",
    "* `classifier.predict_proba(X_new)` takes new data (one row or many) and predicts 'some notion of confidence'.\n",
    "\n",
    "In the case of binary classification (i.e. two classes) the `classifier.predict_proba` usually returns a number where higher numbers indicate some measure of 'confidence'.\n",
    "\n",
    "The `classifier.predict(X_new)` is basically a wrapper around the `predict_proba` function which simply applies a decission threshold at some value (usually 0.5).\n",
    "This is exactly what we did in the case of linear least squares regression.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\begin{cases}\n",
    "\\text{Yes}, & \\text{if $ f(X) \\gt 0.5$} \\\\\n",
    "\\text{No}, & \\text{if $ f(X) \\le 0.5$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In this case the decission threshold corresponds to the distance of a point to the seperating hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does this say about the actual probability of a new data point being of either class?\n",
    "\n",
    "In the case above, data created by two gaussian distributions, the distance certainly maps to the underlying probability density in *some* way. But it is in no way an actual *significance* or *confidence*\n",
    "\n",
    "The function `predict_proba` is a slight misnomer. While some classifiers return numbers between 0 and 1, by no means do all classifier return the desired probability estimate.\n",
    "\n",
    "Still the number can be interpreted as some level of 'certainty' in many cases.\n",
    "\n",
    "Varying the decission threshold is extremely usefull for modifying your classifier output to create more/less 'conservative' predictions.\n",
    "\n",
    "\n",
    "In essence this is a new classifier/predictor with a free parameter. The old $\\hat{y} = \\hat{f}(x)$ now becomes\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{f}(x, \\alpha)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a parameter which can be chosen freely (or optimized according to some criterion which has nothing to do with the underlying loss function of the predictor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1500, noise=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='winter_r')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "plots.draw_decission_surface(clf, predictions, label=r'$ \\alpha $')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Calibration\n",
    "\n",
    "There are ways to transform the output of a classifiers into more reasonable probability estimates. \n",
    "\n",
    "This process is often called classifier calibration. There is a detailed guide in sklearn's documentation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/calibration.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Receiver Operating Characteristic \n",
    "\n",
    "The decission threshold is incredibly helpful in validating classifier performance.\n",
    "\n",
    "The plot of false positive rate vs. true positive rate while varying the decission threshold is called the Receiver Operating Characteristic curve (ROC curve).\n",
    "\n",
    "It is a very popular tool for classifier performance evaluation.\n",
    "\n",
    "Wikipedia Quote:\n",
    "\n",
    "> Following the attack on Pearl Harbor in 1941, the United States army began new research to increase the prediction of correctly detected Japanese aircraft from their radar signals. For this purposes they measured the ability of radar receiver operators to make these important distinctions, which was called the Receiver Operating Characteristics.\n",
    "\n",
    "\n",
    "A classifier which assigns random labels to the data will have a ROC curve which lies on the diagonal. With an area under curve (AUC) of 0.5.\n",
    "\n",
    "\n",
    "__Problems__\n",
    "\n",
    "There are circumstance in which the ROC is not a good measure of quality. \n",
    "\n",
    "See https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve for a discussion.\n",
    "\n",
    "Also in the case of imbalanced classes (eg. imagine you had 10000 surviors but 10 deceased passengers) the ROC curve itself won't change. The intepretation of the ROC curve however changes drastically.\n",
    "\n",
    "In that case its better to plot the precission vs recall curve and the corresponding area under curve.\n",
    "\n",
    "See https://classeval.wordpress.com for some very good discussions on classifier evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=10, n_informative=2, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "prediction = SVC(probability=True).fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, t = roc_curve(y_test, prediction)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(fpr, tpr, c=t, cmap='viridis', s=50)\n",
    "plt.colorbar(label='Decission Threshold')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "X, y = make_classification(n_samples=1000)\n",
    "\n",
    "prediction = np.random.uniform(size=len(y))\n",
    "\n",
    "fpr, tpr, t = roc_curve(y, prediction)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(fpr, tpr, c=t, cmap='viridis')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "We have used a decission tree to classifiy artificial data as well as the Titanic data. \n",
    "\n",
    "Theoretically a decission tree is not limited in its depth. \n",
    "\n",
    "This quickly leads to overfitted tree models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=5000, noise=0.30)\n",
    "clf = DecisionTreeClassifier(max_depth=300, criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=3, cmap='winter')\n",
    "plots.draw_decission_boundaries(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the discussion about decission trees earlier, the tree building algorithms try to find the optimal split criterion in some local region of the parameter space.\n",
    "\n",
    "Finding the best overal split in parameter space is computationaly infeasible.\n",
    "\n",
    "This means the decission tree algorithm can run into a local optimum. \n",
    "\n",
    "The idea of _ensemble learning_ is to train several weak (high bias, low variance) base classifiers on different subsets of the data and then combine them into one big classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging\n",
    "\n",
    "A popular way to build ensembles is called *bagging*.\n",
    "\n",
    "Split the training data into $B$ subsets using sampling with replacement (Bootstrapping). For each subset $b$ we train a classifier $\\hat{f}_b$. Bagging then combines the overall prediction by taking the average.  \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}_b (x) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "Random Forests are a modification to bagging in which a number of *randomized decission trees* are trained. These randomized decission trees use a random subset of variables to find the best split in each node.\n",
    "\n",
    "        def build_random_tree(space)\n",
    "            if stopping_criterion_fulfilled():\n",
    "                return {}\n",
    "            \n",
    "            random_variable_choice = choose_random_selection_of_variables()\n",
    "            variable, split_point = find_best_split(space, random_variable_choice)\n",
    "                        \n",
    "            left, right = split_space(space, variable, split_point)\n",
    "\n",
    "            left_tree = build_tree(left)\n",
    "            right_tree = build_tree(right)\n",
    "               \n",
    "            return {'node' : (variable, split_point), 'left': left_tree, 'right': right_tree}\n",
    "\n",
    "Random Forests are a very popular choice for classification tasks since their parameters can be easily tuned and they often outperform other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = read_titanic()\n",
    "\n",
    "X = data[['Sex_Code', 'Pclass_Code', 'Fare', 'Age', 'SibSp']]\n",
    "y = data['Survived_Code']\n",
    "\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=5)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=20, min_samples_leaf=5)\n",
    "\n",
    "score = cross_validate(tree, X, y, scoring=make_scorer(roc_auc_score), cv=5)\n",
    "print('ROC AUC Decission Tree {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "score = cross_validate(rf, X, y, scoring=make_scorer(roc_auc_score), cv=5)\n",
    "print('ROC AUC Random Forest {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Regression and classification are very similar in nature. The biggest difference being that the target variable $y$ is continous and has a natural ordering associated with it. \n",
    "\n",
    "The same basic rules for classification apply for regression as well. \n",
    "\n",
    "* Models need to be verified on independent test data\n",
    "* There is a tradeoff between bias and variance. Overfitting can occur.\n",
    "* There are many quality measures to pick from. \n",
    "\n",
    "\n",
    "Lets try and use regression to predict housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from IPython.display import Markdown, display\n",
    "houses = load_boston()\n",
    "\n",
    "display(Markdown(houses.DESCR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regression task is to predict the price of a house from the 13 given variables. From a plot of some of the variables we can see some weak correlations in some variables.\n",
    "\n",
    "None of these variables alone would suffice to build a good predictor. Hence we try to use multivariate regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "reg = LinearRegression()\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score linear regression {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "reg = SVR(kernel='rbf')\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score SVR {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score Tree {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "score = cross_validate(reg, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print('Score RandomForestRegressor {:0.3f} +/- {:0.3f}'.format(score['test_score'].mean(), score['test_score'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(houses.data, houses.target, test_size=0.5)\n",
    "reg.fit(X_train, y_train)\n",
    "prediction = reg.predict(X_test)\n",
    "\n",
    "bin_edges = np.linspace(0, 60, 30)\n",
    "plt.hist2d(prediction, y_test, bins=bin_edges, cmap='gray_r',)\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.plot([0, 60], [0, 60], color='gray')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the predictor shows stronger correlation than any of the other variables. It is still noisy and shows some smearing and outliers. The output of a regressor should never be understood as 'true' but rather as a new correlated variable which can show noise and bias. \n",
    "\n",
    "There is a fundemental difference between *curve fitting* and regression.\n",
    "\n",
    "* __Curve Fitting__ All data is available. Some known (or presumed) analytical function is fit to the data to estimate free parameters of that function.\n",
    "\n",
    "\n",
    "* __Regression__ Training data is available. A model is fitted on training data to predict the dependent variable on some new, unknown, data.\n",
    "\n",
    "There is a lot more to learn about linar models and regression. Check sklearn's user guide on linear models for more information \n",
    "http://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "A very popular regression metric is the $R^2$ score. Read about it here\n",
    "\n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "and here\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Material\n",
    "\n",
    "Read scikit-learns's user guide. It's detailed and describes pros and cons of many alogirthms and evaluation criteria.\n",
    "Its also full of code examples.\n",
    "\n",
    "http://scikit-learn.org/stable/user_guide.html\n",
    "\n",
    "Read the book by Hastie (if you're a crazy maths person)\n",
    "\n",
    "http://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Read this book by James and Hastie (if you're a normal person)\n",
    "\n",
    "http://www-bcf.usc.edu/~gareth/ISL/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
