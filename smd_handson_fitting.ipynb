{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 3rem; font-weight: bold;\">SMD Hands-On Schätzen</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "plt.rcParams['figure.figsize'] = (7.5, 5)\n",
    "plt.rcParams['figure.constrained_layout.use'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_to_corr(cov):\n",
    "    '''Convert covariance to correlation matrix\n",
    "    \n",
    "    Taken from: https://math.stackexchange.com/a/300775/892886\n",
    "    '''\n",
    "    D = np.diag(1 / np.sqrt(np.diag(cov)))\n",
    "    return D @ cov @ D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methode der kleinsten Quadrate\n",
    "\n",
    "## Analytische Lösung für Linearkombinationen von Funktionen\n",
    "\n",
    "(Analog zu Aufgabe 27 des letzten Übungszettels)\n",
    "\n",
    "Hier werden wir eine Funktion der Form\n",
    "\n",
    "$$\n",
    "f(x) = p_0 + p_1 \\cdot \\sin(x) + p_2 \\cdot \\cos(x)\n",
    "$$\n",
    "\n",
    "an Daten anpassen.\n",
    "\n",
    "Diese Funktion ist eine Linearkombination von Basisfunktionen:\n",
    "$$\n",
    "f(x) = \\sum_{i=0}^2 p_i f_i(x)\n",
    "$$\n",
    "\n",
    "mit \n",
    "\n",
    "$$\n",
    "f_0(x) = 1, \\quad f_1(x) = \\sin(x), \\quad f_2(x) = \\cos(x)\n",
    "$$\n",
    "\n",
    "Für diesen Fall gilt die in der Vorlesung und den Übungen vorgestellte analytische Lösung für das Problem der Kleinsten Quadrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def linear_combination(x, funcs, parameters):\n",
    "    '''Evaluate a liner combination of base functions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: number or np.ndarray\n",
    "        The point or points at which to evaluate\n",
    "    funcs: iterable of callables\n",
    "        The base functions\n",
    "    parameters: iterable of numbers\n",
    "        The coefficients\n",
    "    '''\n",
    "    if len(funcs) != len(parameters):\n",
    "        raise ValueError('len(funcs) must be equal to len(parameters)')\n",
    "        \n",
    "    return np.sum([p * f(x) for p, f in zip(parameters, funcs)], axis=0)\n",
    "\n",
    "\n",
    "funcs = [np.ones_like, np.sin, np.cos]\n",
    "\n",
    "# create some randomized example data points\n",
    "rng = np.random.default_rng(1337)\n",
    "N = 100\n",
    "true_parameters = np.array([2, 1, 0.5])\n",
    "y_unc = rng.uniform(0.1, 0.4, N)\n",
    "\n",
    "x = np.linspace(0, 4 * np.pi, N)\n",
    "y = linear_combination(x, funcs, true_parameters)\n",
    "\n",
    "# Messunsicherheit simulieren\n",
    "y += rng.normal(0, y_unc)\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=y_unc, ls='', marker='.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufstellen der Design Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.column_stack([f(x) for f in funcs])\n",
    "A[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufstellen der Gewichtsmatrix $\\boldsymbol{W} = \\mathrm{Cov}^{-1}(\\mathbf{y})$ der Messwerte.\n",
    "\n",
    "Wir gehen hier davon aus, dass wir die Unsicherheit jedes Messpunktes kennen und das keine Korrelation zwischen den Punkten existiert. Dies ist eine starke Vereinfachung.\n",
    "\n",
    "Die Methode der kleinsten Quadrate erzeugt verzerrte Ergebnisse, falls die Unsicherheiten aus den Messpunkten selbst geschätzt werden. Mehr dazu weiter unten in dem Myon-Beispiel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All measurements have a known uncertainty and no correlations\n",
    "W = np.diag(np.full(N, 1 / y_unc**2)) \n",
    "plt.matshow(W)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berechnung der Lösung und der Kovarianz der Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.linalg.inv(A.T @ W @ A)\n",
    "\n",
    "parameters = cov @ A.T @ W @ y\n",
    "parameters, cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-Quadrad-Über-Anzahl-Freiheitsgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = (y - A @ true_parameters)\n",
    "sum_residuals = (residuals.T @ W @ residuals)\n",
    "\n",
    "ndf = len(y) - len(true_parameters)\n",
    "chisquare_ndf = sum_residuals / ndf\n",
    "chisquare_ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fit = np.linspace(0, 4 * np.pi, 1000)\n",
    "y_fit = linear_combination(x_fit, funcs, parameters)\n",
    "y_truth = linear_combination(x_fit, funcs, true_parameters)\n",
    " \n",
    "\n",
    "plt.figure()\n",
    "plt.title(\n",
    "    rf'$f(x) = {parameters[0]:.2f} + {parameters[1]:.2f} \\cdot \\sin(x) + {parameters[2]:.2f} \\cdot \\cos(x)'\n",
    "    rf', \\quad \\chi^2_\\mathrm{{ndf}} = {chisquare_ndf:.2f}$'\n",
    ")\n",
    "plt.errorbar(x, y, yerr=y_unc, ls='', marker='.', label='Daten')\n",
    "plt.plot(x_fit, y_fit, label='Fit-Ergebnis')\n",
    "plt.plot(x_fit, y_truth, label='Wahrheit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cov_to_corr(cov)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "m = ax.matshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "fig.colorbar(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerische Lösung für allgemeine Funktionen\n",
    "\n",
    "\n",
    "Ist die anzupassende Funktion *keine* Linearkombination einzelner Funktionen, kann die Lösung\n",
    "nur numerisch gefunden werden.\n",
    "\n",
    "Aus dem Praktikum bekannt ist die Funktion `scipy.optimize.curve_fit`, die genau dies tut.\n",
    "\n",
    "Der verwendete Nelder-Mead-Algorithmus hat die Eigenschaft, garantiert die korrekte analytische Lösung zu finden, falls sie existiert.\n",
    "\n",
    "- https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n",
    "\n",
    "### Anwendung auf lineares Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def func(x, p1, p2, p3):\n",
    "    return p1 + p2 * np.sin(x) + p3 * np.cos(x)\n",
    "\n",
    "\n",
    "# absolute_sigma prevents scaling of errors to match χ²/ndf=1\n",
    "parameters_numeric, cov_numeric = curve_fit(\n",
    "    func,x, y,\n",
    "    sigma=np.full(N, y_unc),\n",
    "    absolute_sigma=True,\n",
    ")\n",
    "\n",
    "print(parameters, parameters_numeric, cov, cov_numeric, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fit = np.linspace(0, 4 * np.pi, 1000)\n",
    "y_fit = linear_combination(x_fit, funcs, parameters)\n",
    "y_num = linear_combination(x_fit, funcs, parameters_numeric)\n",
    "y_truth = linear_combination(x_fit, funcs, true_parameters)\n",
    " \n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(x, y, yerr=y_unc, ls='', marker='.', label='Daten')\n",
    "plt.plot(x_fit, y_fit, label='Fit-Ergebnis Analytisch')\n",
    "plt.plot(x_fit, y_num, label='Fit-Ergebnis Numerisch', linestyle=':')\n",
    "plt.plot(x_fit, y_truth, label='Wahrheit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum dann nicht einfach immer die numerische Variante nehmen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "A = np.column_stack([f(x) for f in funcs])\n",
    "\n",
    "ATW = A.T @ W\n",
    "cov = np.linalg.inv(ATW @ A)\n",
    "parameters = cov @ ATW @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "parameters_numeric, cov_numeric = curve_fit(func, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel mit allgemeiner Funktion \n",
    "\n",
    "Aus dem Praktikum, Spaltfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/spalt.csv')\n",
    "\n",
    "\n",
    "LASER_WAVELENGTH_NM = 632.8e-9\n",
    "\n",
    "def theory(phi, A0, b):\n",
    "    return (A0 * b * np.sinc(b * np.sin(phi) / LASER_WAVELENGTH_NM))**2\n",
    "\n",
    "\n",
    "# first try with default initial guess (1 for every parameter)\n",
    "p0 = None\n",
    "\n",
    "# now with an \"educated guess\" based on the data and knowledge of the\n",
    "# order of magnitude of the slit size\n",
    "p0 = [np.sqrt(df['I'].max()) / 1e-4, 1e-4]\n",
    "\n",
    "params, cov = curve_fit(theory, df['phi'], df['I'], p0=p0)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(-0.03, 0.03, 501)\n",
    "plt.plot(x, theory(x, *params), label='Fit')\n",
    "\n",
    "plt.plot(df['phi'], df['I'], '.', label='Daten')\n",
    "\n",
    "plt.xlabel(r'$\\varphi \\,\\, / \\,\\, \\mathrm{rad}$')\n",
    "plt.ylabel(r'$I \\,\\, / \\,\\, \\mathrm{A}$')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "None # to not have mpl objects in the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximimum-Likelihood-Methode\n",
    "\n",
    "## Ungebinnter Fit von Wahrscheinlichkeitsdichten\n",
    "\n",
    "\n",
    "Stark vereinfachtes Beispiel einer CERN-Analyse.\n",
    "\n",
    "Wir suchen den Massenpeak eines Teilchens (normal-verteilt) und haben einen exponential-verteilten Untergrund.\n",
    "\n",
    "Wir erzeugen wieder einen \"Spielzeug\"-Datensatz mit Monte-Carlo Methoden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "E_MIN = 75\n",
    "E_MAX = 175\n",
    "\n",
    "# normally distributed signal\n",
    "higgs_signal = np.random.normal(126, 5, 500)\n",
    "\n",
    "# exponentially distributed background\n",
    "background = np.random.exponential(50, size=20000)\n",
    "\n",
    "# combine signal and background\n",
    "measured = np.append(higgs_signal, background)\n",
    "\n",
    "# remove events outside of \"detector range\"\n",
    "in_range = (E_MIN <= measured) & (measured <= E_MAX)\n",
    "measured = measured[in_range]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(measured, bins=100)\n",
    "plt.xlabel('$m \\,/\\, \\mathrm{GeV}$')\n",
    "plt.margins(x=0)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition der negativen Log-Likelihood\n",
    "\n",
    "Wir bilden eine Überlagerung von zwei Wahrscheinlichkeitsdichten, die einen Anteil von $p$ bzw. $1-p$ haben.\n",
    "\n",
    "Wir müssen die Wahrscheinlichkeitsdichten auf das gemessene Intervall normieren.\n",
    "In diesem Fall ignorieren wir dies für die Normalverteilung unter der Annahme, dass der Massenpeak vollständig im Intervall enthalten ist.\n",
    "\n",
    "Also:\n",
    "\n",
    "\\begin{align}\n",
    "P_1 &= N(\\mu, \\sigma) \\\\[1ex]\n",
    "P_2 &= \\frac{1}{\\exp(-E_\\mathrm{min} / \\tau) - \\exp(-E_\\mathrm{max} / \\tau)} \\exp(- E / \\tau) \\\\[1ex]\n",
    "P(E | p, \\mu, \\sigma, \\tau) &= p \\cdot P_1(E, \\mu, \\sigma) + (1 - p) P_2(E | \\tau)) \\\\[1ex]\n",
    "\\mathcal{L}(p, \\mu, \\sigma, \\tau) &= \\prod_i P(E_i | p, \\mu, \\sigma, \\tau) \\\\[1ex]\n",
    "-\\log\\mathcal{L}(p, \\mu, \\sigma, \\tau) &= -\\sum_i \\log(P(E_i | p, \\mu, \\sigma, \\tau))\n",
    "\\end{align}\n",
    "\n",
    "Im Code, unter Verwendung der Verteilungs-Klassen aus `scipy.stats`, sieht die PDF wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon\n",
    "\n",
    "\n",
    "def pdf(x, mean, std, tau, p):\n",
    "    expon_norm = np.exp(-E_MIN / tau) - np.exp(-E_MAX / tau)\n",
    "    return (\n",
    "        p * norm.pdf(x, mean, std) \n",
    "        + (1 - p) / expon_norm * expon.pdf(x, scale=tau)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lösung mit `scipy.optimize.minimize` und `numdifftools.Hessian`\n",
    "\n",
    "\n",
    "Mit `scipy.optimize.minimize` können allgemeine Funktionen minimiert werden.\n",
    "\n",
    "Diese müssen als erstes Argument ein array der zu variierenden Parameter haben.\n",
    "Weitere Parameter können über das `args` Argument von `minimize` übergeben werden.\n",
    "\n",
    "Naiv sieht dann unsere negative Log-Likelihood für die Minimierung mit scipy so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(parameters, data):\n",
    "    return -np.sum(np.log(pdf(data, *parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naiv, weil häufig analytische Umformungen der der Log-Likelihood möglich sind,  \n",
    "die zu weniger Operationen führen und somit sowohl Rechenzeit sparen als auch Rundungsfehler vermeiden.\n",
    "\n",
    "Es macht also in vielen Fällen Sinn, die Log-Likelihood per Hand aufzuschreiben und so weit es geht zu vereinfachen. Zum Beispiel auch durch das Weglassen von Termen die unter Variation der Parameter konstant sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir finden ein Minimum (und hoffentlich das globale) mit `scipy.optimize.minimize`. \n",
    "\n",
    "Wie aus der Vorlesung bekannt, bekommen wir einen Schätzer für die Covarianz unserer Parameter aus der Inversen der Hesse-Matrix der Likelihood, ausgewertet im Minimum.\n",
    "\n",
    "Die Hesse-Matrix muss numerisch bestimmt werden. Leider ist die Hesse-Matrix die `scipy.optimize.minimize` mit dem Ergebnis zusammen zurück gibt nur eine grobe Schätzung.\n",
    "\n",
    "Wir nutzen `numdifftools.Hessian` um numerisch die Hesse-Matrix zu bestimmen.\n",
    "\n",
    "`scipy.optimize.minimize` unterstützt mehrere Minimierungs-Algorithmen mit unterschiedlichen Fähigkeiten und Stärken. \n",
    "Für den Fall, dass man Grenzen für die Parameter (`bounds=`) angibt, ist der default der `L-BFGS-B` Algorithmus.\n",
    "\n",
    "Dokumentation: \n",
    "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "* https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# a very small number to achive > 0 instead of >= 0\n",
    "eps = np.finfo(np.float64).eps\n",
    "\n",
    "result = minimize(\n",
    "    neg_log_likelihood,\n",
    "    args=(measured, ),\n",
    "    x0=[130, 2, 30, 0.2], # here, the initial guess is required\n",
    "    bounds=[\n",
    "        (0, None),    # mean >= 0\n",
    "        (eps, None),  # std > 0\n",
    "        (eps, None),  # tau > 0\n",
    "        (0, 1),       # 0 <= p <= 1\n",
    "    ],\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numdifftools import Hessian\n",
    "\n",
    "hesse = Hessian(neg_log_likelihood)\n",
    "cov = np.linalg.inv(hesse(result.x, measured))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs_mass = result.x[0]\n",
    "higgs_mass_unc = np.sqrt(cov[0, 0])\n",
    "print(f'Higgs mass is {higgs_mass:.2f} ± {higgs_mass_unc:.2} GeV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lösung mit iminuit\n",
    "\n",
    "\n",
    "Iminuit ist ein Python Paket, dass Zugriff auf den Minimierungsalgorithmus \"Minuit\" aus dem ROOT Framework ermöglicht.\n",
    "\n",
    "Hierzu ist keine Installation von ROOT notwendig. \n",
    "\n",
    "\"Minuit\" gilt – zumindest unter Teilchenphysiker:innen – als das \"Non-Plus-Ultra\" der Minimierer.\n",
    "\n",
    "Siehe dazu auch: https://www.babushk.in/posts/adventures-physics-software-minuit.html \n",
    "\n",
    "`iminuit` stellt den Minimierer und Loss-Funktionen zur Verfügung, die es wesentlich einfacher machen,  \n",
    "verschiedene Arten von Fits durchzuführen.\n",
    "\n",
    "Außerdem berechnet es die Unsicherheiten über die Hesse-Matrix und stellt auch über das \"minos\" Interface Likelihood-Scans zur genaueren Bestimmung von Unsicherheiten über Likelihood-Ratio Tests zur Verfügung (dazu mehr in den nächsten Vorlesungen zum Thema Testen). \n",
    "\n",
    "\n",
    "\n",
    "Außerdem stellt es – gerade im Notebook – schöne Übersichten über den Fit-Vorgang zur Verfügung.\n",
    "\n",
    "Für den Beginn einmal das gleiche Problem wie oben, nun gelöst mit `imuit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "from iminuit.cost import UnbinnedNLL\n",
    "\n",
    "# minuit's UnbinnedNLL takes directly the pdf and the observed data\n",
    "loss = UnbinnedNLL(measured, pdf)\n",
    "\n",
    "m = Minuit(loss, mean=130, std=2, tau=30, p=0.2)\n",
    "\n",
    "# set bounds\n",
    "m.limits['mean'] = (0, None)   # >= 0\n",
    "m.limits['std'] = (eps, None)  # > 0\n",
    "m.limits['tau'] = (eps, None)  # > 0\n",
    "m.limits['p'] = (0, 1)\n",
    "\n",
    "# perform minimization\n",
    "m.migrad()\n",
    "\n",
    "# perform likelihood scan for confidence intervals\n",
    "m.minos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs_mass = m.values['mean']\n",
    "higgs_mass_unc = m.errors['mean']\n",
    "\n",
    "print(f'Higgs mass is {higgs_mass:.2f} ± {higgs_mass_unc:.2} GeV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson-Likelihood-Fit einer histogrammierten Ereignisverteilung\n",
    "\n",
    "\n",
    "Stehen die einzelnen Beobachtungen nicht zur Verfügung oder ist bei der Analyse von großen Datenmengen die Laufzeit relevant, kann auch ein sogenannter \"gebinnter\" Fit durchgeführt werden.\n",
    "\n",
    "Hier wird die Wahrscheinlichts-Verteilung an das Histogramm angepasst.\n",
    "\n",
    "Da ein Histogramm in jedem Bin ein Zählexperiment darstellt, erwarten wir eine Poisson-Statistik für jedes Bin.\n",
    "\n",
    "Die kumulierte Wahrscheinlichkeitsdichte liefert zusammen mit der Gesamtereignis-Anzahl den Erwartungswert für jedes Histogramm-Bin, abhängig von den zu fittenden Parametern $\\boldsymbol{\\theta}$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{i=1}^N \\mathcal{P}(k=H_i, \\lambda=\\lambda_i(\\boldsymbol{\\theta}))\n",
    "$$\n",
    "\n",
    "mit \n",
    "\n",
    "$$\n",
    "\\lambda_i = N_\\mathrm{total} \\cdot (\\mathrm{CDF}(b_i, \\boldsymbol{\\theta}) - \\mathrm{CDF}(a_i, \\boldsymbol{\\theta}))\n",
    "$$\n",
    "\n",
    "Wobei $a_i$ und $b_i$ die Bin-Grenzen des $i$-ten Bins sind. Wir integrieren also die PDF in den einzelnen Bins und multiplizieren mit der Gesamtzahl der Ereignisse.\n",
    "\n",
    "\n",
    "**Beispiel aus Lebensdauer kosmische Myonen (V01)**\n",
    "\n",
    "Im Experiment wird direkt in Hardware ein Histogramm gemessen. \n",
    "\n",
    "Die einzelnen Lebensdauern stehen also nicht zur Verfügung.\n",
    "\n",
    "Die Anleitung fordert einen Least-Squares-Fit and die Bin-Höhen. \n",
    "Dies liefert einen Erwartungstreuen Schätzer, so lange ein ungewichteter Fit durchgeführt wird.\n",
    "\n",
    "In der Vergangenheit wurde empfohlen, einen gewichteten Fit unter der Annahme $\\sigma_i = \\sqrt{H_i}$ durchzuführen.\n",
    "\n",
    "Dies ist falsch! Diese Methode liefert konsistent verzerrte Ergebnisse, da Unterfluktuationen stärker gewichtet werden als Überfluktuationen.\n",
    "\n",
    "Die korrekte Methode ist der gebinnte Poisson-Likelihood Fit wie hier oder ein iteratives Least-Squares-Verfahren wie es in SMD-2 vorgestellt wird.\n",
    "\n",
    "Vergleich der Methoden hier: https://gist.github.com/maxnoe/41730e6ca1fac01fc06f0feab5c3566d\n",
    "\n",
    "In dem Versuch kommt es zusätzlich zu der erwarteten Exponential-Verteilung der Myon-Zerfälle noch zu einem gleichverteilten Untergrund durch koinzidente, nicht-zerfallende Myonen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = np.genfromtxt(\"resources/muon_data.txt\")\n",
    "t = np.arange(len(N) + 1) / 21.48\n",
    "\n",
    "t = t[5:]\n",
    "N = N[5:]\n",
    "\n",
    "plt.figure()\n",
    "plt.stairs(values=N, edges=t)\n",
    "plt.xlabel('t / µs')\n",
    "plt.ylabel('Ereignisanzahl')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iminuit stellt für diesen Anwendungsfall die `BinnedNLL` Loss-Funktion zur Verfügung,\n",
    "die genau die Poisson-Likelihood für ein gemessenes Histogram darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit.cost import BinnedNLL\n",
    "from scipy.stats import uniform\n",
    "\n",
    "T_MIN, T_MAX = t[0], t[-1]\n",
    "\n",
    "def cdf(x, tau, p):\n",
    "    # normalize to 1 in histogram range\n",
    "    cdf_min, cdf_max = expon.cdf([T_MIN, T_MAX], scale=tau) \n",
    "    norm = 1 / (cdf_max - cdf_min)\n",
    "    \n",
    "    # combine exponential signal with uniform background\n",
    "    return (\n",
    "        p * expon.cdf(x, scale=tau) * norm\n",
    "        + (1 - p) * uniform.cdf(x, T_MIN, T_MAX)\n",
    "    )\n",
    "\n",
    "\n",
    "# histogram counds, histogram edges and cumulative distribution function\n",
    "loss = BinnedNLL(N, t, cdf)\n",
    "\n",
    "m = Minuit(loss, tau=2, p=0.99)\n",
    "m.limits['tau'] = (eps, None)\n",
    "m.limits['p'] = (0, 1)\n",
    "m.migrad()\n",
    "m.minos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muon_lifetime = m.values['tau']\n",
    "muon_lifetime_unc = m.errors['tau']\n",
    "\n",
    "print(f'Fit: τ = {muon_lifetime:.3f} ± {muon_lifetime_unc:.3f} µs')\n",
    "print(f'Lit: τ = 2.1969811 ± 0.0000022 µs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood-Scan für die Unsicherheiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, ts, valid = m.mnprofile('tau', size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.axvline(m.values['tau'], color='C1')\n",
    "plt.plot(m.values['tau'], m.fval, color='C1', marker='o', zorder=3)\n",
    "\n",
    "plt.axvline(m.values['tau'] + m.merrors['tau'].lower, color='C2')\n",
    "plt.axvline(m.values['tau'] + m.merrors['tau'].upper, color='C2')\n",
    "plt.axhline(m.fval + 1, color='C3')\n",
    "\n",
    "plt.plot(tau, ts)\n",
    "\n",
    "plt.xlabel('τ / µs')\n",
    "plt.ylabel('NLL')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "m.draw_mncontour('tau', 'p', size=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
