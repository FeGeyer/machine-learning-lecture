{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA und Dimensionsreduzierung\n",
    "\n",
    "> __In God we trust, all others bring data__\n",
    "\n",
    ">William Edwards Deming (1900-1993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format der \"Folien\"\n",
    "\n",
    "Hierbei handelt es sich um ein sogenanntes Jupyter Notebook.\n",
    "\n",
    "http://jupyter.org/\n",
    "\n",
    "Eine Mischung aus Code, Bildern und Text mit Teilweise interaktiven Elementen.\n",
    "Auch lokal auf euren Laptops ausführbar. \n",
    "\n",
    "<img src=\"./ml/images/jupyter.png\" alt=\"Jupyter Logo\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## scikit-learn (sklearn)\n",
    "\n",
    "Programmbesipiele und plots in der Vorlesung sind mithilfer des scikit-learn Projektes erstellt.\n",
    "\n",
    "[http://scikit-learn.org/stable](http://scikit-learn.org/stable)\n",
    "\n",
    "scikit-learn ist eine Bibliothek für Python mit vielen fertigen Methoden und Algortihmen für Data Mining / Machine Learning\n",
    "\n",
    "Ausführlicher User-Guide mit Beispielen und mathematischen Hintegründen\n",
    "\n",
    "[http://scikit-learn.org/stable/user_guide.html](http://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "<img src=\"./ml/images/logo.png\" alt=\"Scikit Logo\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Installation (wie immer)\n",
    "\n",
    "    pip install sklearn\n",
    "    \n",
    "oder \n",
    "    \n",
    "    conda install sklearn\n",
    "\n",
    "\n",
    "![sklearn citations](./ml/images/sklearn_citations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notationen\n",
    "\n",
    "*Für weiteres siehe \"Elements of statistical Learning\" von Trevor Hastie. https://web.stanford.edu/~hastie/ElemStatLearn/ (Kostenloses E-Book)* \n",
    "\n",
    "Ich versuche folgenden Namenskonventionen zu folgen.\n",
    "\n",
    "* Großbuchstaben wie  $X$ oder $Y$ bezeichnen generische Aspekte einer Variable (i.e. die tatsächliche Zufallsvariable)\n",
    "* Beobachtungen/Realisierungen werden klein geschrieben. Die i-te Realisierung in $X$ ist $x_i$\n",
    "* Matrizen sind groß- und fettgedruckt $\\mathbf{X}$\n",
    "* Beobachtungen/Realisierungen sind *Zeilen* der Matrix während die beobachteten Größen in den *spalten* stehen.\n",
    "\n",
    "Wenn man beispielsweise $d=2$ Variablen, das Alter und das Gewicht von $N = 100$ Menschen misst, dann erhält man eine $N \\times d$ Matrix $\\mathbf{X}$.\n",
    "\n",
    "eine Beobachtung, bzw. Zeile, der Matrix wird geschrieben als $x_i = [ Alter, Gewicht ]$. Alle Messungen der Größe *Gewicht* sind geschrieben als  $\\mathbf{x}_1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ml import plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "discrete_cmap = ListedColormap([(0.8, 0.2, 0.3), (0.1, 0.8, 0.3), (0, 0.4, 0.8)])\n",
    "\n",
    "sns.reset_orig()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['xtick.labelsize'] = 13\n",
    "plt.rcParams['ytick.labelsize'] = 13\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiederholung\n",
    "\n",
    "Beim letzten mal:\n",
    "\n",
    "Lineare Fisher Diskriminanzanalyse:\n",
    "\n",
    "> Finde die Hyperebenen welche zwei Populationen optimal nach Fisher Kriterium trennt.\n",
    "\n",
    "Das kleine Beispiel unten zeigt wie eine Diskriminanzanalyse mit dem scikit-learn Paket durchgeführt werden kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X, y = make_blobs(n_samples=250, centers=2, random_state=12)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "lda = clf.fit(X, y)\n",
    "plots.draw_linear_regression_function(lda, color='gray')\n",
    "plt.axis('off')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probleme bei hochdimensionalen Daten:\n",
    "\n",
    "> __Curse of dimensionality (Fluch der Dimensionalität)__\n",
    "\n",
    ">ist ein Begriff, der von Richard Bellman eingeführt wurde, um den rapiden Anstieg im Volumen beim Hinzufügen weiterer Dimensionen in einen mathematischen Raum zu beschreiben.\n",
    "\n",
    ">[https://de.wikipedia.org/wiki/Fluch_der_Dimensionalität](https://de.wikipedia.org/wiki/Fluch_der_Dimensionalit%C3%A4t)\n",
    "\n",
    "Je höher die Dimension des Raumes, umso mehr Beobachtungen braucht man um den Raum *ausreichend* abzudecken.\n",
    "\n",
    "Im folgenden Beispiel werden 100 Punkte aus eine uniformen Verteilung zwischen 0 und 1 gezogen.  \n",
    "Darunter wird das Histogram gezeichnet.  \n",
    "Zunächst in einer Dimension und dann in zwei.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(sample, np.zeros_like(sample), '.')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.hist(sample, bins=np.arange(0, 1.1, 0.1),histtype='step', lw=2)\n",
    "ax2.set_ylabel('Häufigkeit')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#gleichverteilte Zahlen in zwei Dimensionen\n",
    "sample = np.random.uniform(low=0.0, high=1.0, size=[100, 2])\n",
    "\n",
    "#einzelne Punkte plotten\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(10, 5))\n",
    "ax1.plot(sample[:, 0], sample[:, 1], '.')\n",
    "ax1.axis('off')\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "# das histogram plotten\n",
    "histogram_entries, _, _, im = ax2.hist2d(\n",
    "    sample[:, 0],\n",
    "    sample[:, 1],\n",
    "    bins=np.arange(0, 1.1, 0.1),\n",
    "    cmap='Blues',\n",
    "    vmin=0,\n",
    ")\n",
    "\n",
    "# anteil besetzter bins bestimmen\n",
    "density = np.count_nonzero(histogram_entries) / histogram_entries.ravel().shape[0]\n",
    "\n",
    "ax2.set_title('Besetzte Bins: {:.2%}'.format(density))\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "fig.colorbar(im, ax=ax2)\n",
    "fig.colorbar(im, ax=ax1).ax.set_visible(False)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im 1D Beispiel ist jeder Bin besetzt. Es gibt keine leeren bins. Im 2D Beispiel sind bereits ungefähr ein Drittel der Bins leer. \n",
    "\n",
    "Lösungsmöglichkeiten:\n",
    "\n",
    "1. Mehr Daten Speichern und höhere Kosten in Kauf nehmen.\n",
    "2. Größere Bins benutzen und die Verteilung weniger genau wiedergeben.\n",
    "3. Dimensionen reduzieren und eventuell Informationen verwerfen. \n",
    "\n",
    "Mehr Daten zu speichern ist auch heutzutage nicht immer möglich.\n",
    "\n",
    "###### Beispiel IceCube\n",
    "Der IceCube Neutrino Detektor am Südpol nimmt bis zu 1 TB Daten am Tag auf. \n",
    "Per Satelit können nur 100 GB pro Tag übetragen werden.\n",
    "Die restlichen Daten werden einmal pro Jahr per Schiff versandt.\n",
    "\n",
    "<img src=\"./ml/images/icecube.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "###### Beispiel SKA\n",
    "Das Square Kilometer Array ist ein geplantes Radio Teleskop welches in Südafrika und Australien gebaut werden soll.\n",
    "Es wird aus mehreren zehntausenden Antennen bestehen. Die erwartete Datenrate liegt in der Größenordnung von mehreren  __Petabyte pro Sekunde__. Eine Speicherung ist mit heutiger Technologie völlig unmöglich.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"820\" height=\"480\" src=\"https://www.youtube.com/embed/8BBoDw2qVD0?rel=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hochdimensionale Daten führen aber auch zu ganz Grundsätzlichen, mathematischen,  Problemen.\n",
    "Interessante Diskussion hier: \n",
    "> Why is Euclidean distance not a good metric in high dimensions?\n",
    ">https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions\n",
    "\n",
    "Benötigt wird also irgendeine Art von Datereduktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datenreduktion\n",
    "\n",
    "Zwei grundsätzliche Ansätze:\n",
    "\n",
    " 1. Feature Extraction\n",
    "   \n",
    "       -   Erzeugen neuer Attribute, die Informationen mehrerer Attribute zusammenfassen\n",
    "       -   Transformation des Datenraumes (Hauptkomponenteanalyse)\n",
    "   \n",
    " 2. Feature Selection\n",
    "   \n",
    "       - Verwerfen von redundanten und schwachen Attributen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "#### Einfaches Beispiel für Datentransformation\n",
    "\n",
    "Gegeben seien Datenpunkte $X = (x_1, x_2, ...) , Y = (y_1, y_2, ...) $.\n",
    "\n",
    "Angenommen durch Fachwissen, bekannte physikalische Zusammenhänge oder einfach genaues hinsehen sei bekannt, \n",
    "dass die Daten sich besser durch Polarkoordinaten ausdrücken lassen.\n",
    "\n",
    "$$\n",
    " \\begin{align}\n",
    "     r =& x^2 + y^2 \\\\\n",
    "     \\phi =& arctan2(y, x)    \n",
    " \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def transform(X1, X2):\n",
    "    r = np.sqrt(X1**2 + X2**2)\n",
    "    phi = np.arctan2(X2, X1)\n",
    "    return r, phi\n",
    "\n",
    "X, y = make_circles(n_samples=500, noise=0.15, factor=0.4, )\n",
    "r, phi = transform(X[:, 0], X[:, 1])\n",
    "\n",
    "#erstelle eine figure mit zwei subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#plotte die ursprünglichen Punkte\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y, cmap=discrete_cmap)\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_title('Original')\n",
    "\n",
    "#plotte die transformierten Punkte\n",
    "ax2.scatter(r, phi, c=y, cmap=discrete_cmap)\n",
    "ax2.set_xlabel('r')\n",
    "ax2.set_ylabel('$\\phi$')\n",
    "ax2.set_title('Transformiert')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In diesem Beispiel reicht es völlig allein das Attribut $r$ zu speichern um die Daten in zwei klassen zu unterteilen.\n",
    "\n",
    "Diese Art der Feature Extraction erfordert irgendeine Art von Expertenwissen und wird bei steigender Dimensionalität komplizierter bis unmöglich. \n",
    "\n",
    "Expertenwissen bedeuted häufig, dass bekannte physikalische Zusammenhänge ausgenutzt werden. Manchmal sind es aber gerade diese physikalischen Zusammenhänge die unbekannt sind und gelernt werden sollen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA (Hauptkomponentenanalyse)\n",
    "\n",
    "Die Hauptkomponentenanalyse sucht nach einer Basis im Raum indem die Varianz entlang der Basisvektoren maximiert wird.\n",
    "\n",
    "Gegeben seien also $N$ Datenpunkte mit $d$ Dimensionen die auf $k < d$ Dimensionen transformiert werden sollen.\n",
    "\n",
    "Dazu wird der Raum in eine neue Basis transformiert. Es werden also eventuell mehrere Dimensionen/Attribute zu einer neuen Zusammengefasst.  \n",
    "\n",
    "Grober Ablauf der PCA\n",
    "\n",
    "0. Zentriere die Daten auf ihren Mittelwert.\n",
    "1. Berechne die Kovarianzmatrix aus der Datenmatrix $\\mathbf{X}$\n",
    "2. Berechne Eigenwerte und Eigenvektoren der Matrix\n",
    "3. Wähle die $k$ größten Eigenwerte und zugehörigen Eigenvektoren aus. \n",
    "4. Bilde eine $d \\times k$ Matrix $\\mathbf{W}$ mit den $k$ Eigenvektoren als Spalten.\n",
    "5. Wende $\\mathbf{W}$ auf jede Zeile aus $x$ aus $\\mathbf{X}$ an $x^\\prime = \\mathbf{W}^T \\cdot x^T $ \n",
    "    \n",
    "\n",
    "###### 1. Zentrierung \n",
    "\n",
    "Berechne die Mittelwertvektoren $\\mu$:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu} = \\begin{pmatrix}\n",
    "    \\bar{\\mathbf{x}}_1 \\\\\n",
    "    \\ldots \\\\\n",
    "    \\bar{\\mathbf{x}}_d \\\\\n",
    "\\end{pmatrix}\n",
    " = \\frac 1 N\n",
    " \\begin{pmatrix}\n",
    "    \\sum_{i=0}^{N} \\mathbf{x}_{1, i} \\\\\n",
    "    \\ldots \\\\\n",
    "    \\sum_{i=0}^{N} \\mathbf{x}_{d, i} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Anders ausgedrückt:\n",
    "$$\n",
    "\\mathbf{\\mu} = \\begin{pmatrix}\n",
    "    \\text{Mittelwert aller Beobachtungen von Attribut 1}\\\\\n",
    "    \\ldots \\\\\n",
    "    \\text{Mittelwert aller Beobachtungen von Attribut d}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Neuer Datenpunkt:\n",
    "\n",
    "   $$\n",
    "      x^{\\prime}_i = x_i - \\mathbf{\\mu}\n",
    "   $$\n",
    "\n",
    "###### 2. Kovarianz\n",
    "\n",
    "Die Kovarianz zweier Zufallsvariablen.\n",
    "$$\n",
    "\\operatorname {Cov} (X,Y)=\\operatorname {E} {\\bigl [}(X-\\operatorname {E} (X))\\cdot (Y-\\operatorname {E} (Y)){\\bigr ]}\n",
    "$$\n",
    "\n",
    "Für mehr als zwei Variablen in Matrixschreibweise\n",
    "\n",
    "$$\n",
    "\\operatorname {Cov} (\\mathbf {X} ) = \\operatorname {E} {\\bigl (}(\\mathbf {X} -\\mathbf {\\mu } )(\\mathbf {X} -\\mathbf {\\mu } )^{T}{\\bigr )}\n",
    "$$\n",
    "\n",
    "\n",
    "###### 3. Eigenwerte und Vektoren\n",
    "\n",
    "Berechne die $d$ verschiedenen Eigenwerte der Kovarianzmatrix $ \\operatorname {Cov} (\\mathbf {X} )$.\n",
    "\n",
    "Erhalte Eigenwerte $\\lambda_1, \\ldots, \\lambda_d$ mit passenden Eigenvektoren $v_1, \\ldots, v_d$\n",
    "\n",
    "\n",
    "###### 4. Sortierung und Auswahl\n",
    "\n",
    "Sortiere die Indizes der Eigenwerte und Vektoren so dass gilt \n",
    "\n",
    "$$\n",
    "\\lambda_1 > \\lambda_2 > \\lambda_3 \\ldots > \\lambda_d\n",
    "$$\n",
    "\n",
    "Wähle die $k$ größten Eigenwerte aus und verwerfe alle anderen Eigenwerte und Vektoren.\n",
    "\n",
    "\n",
    "\n",
    "###### 5. Bildung der Matrix\n",
    "\n",
    "Nutze die $k$ ausgewählten Eigenvektoren als Spalten in der Matrix $\\mathbf{W}$\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\begin{pmatrix}\n",
    "    v_1, \n",
    "    \\ldots,  \n",
    "    v_k\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "    v_{1,1}, \n",
    "    \\ldots,  \n",
    "    v_{k, 1} \\\\\n",
    "    \\ldots \\\\\n",
    "        v_{1,d}, \n",
    "    \\ldots,  \n",
    "    v_{k, d}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "###### 6. Transformierung \n",
    "\n",
    "Multipliziere die Transformationsmatrix $\\mathbf{W}$ mit jeder Beobachtung $x_i$ in $\\mathbf{X}$ um die auf $k$ Dimensionen beschränkten Punkte zu erhalten.\n",
    "\n",
    "$$\n",
    "x_i^\\prime = \\mathbf{W}^T \\cdot x_i^T  \n",
    "$$\n",
    "\n",
    "#### Beispiel in 3D\n",
    "\n",
    "Künstlicher Datensatz mit $d = 3$ Dimensionen wird auf $k=2$ Dimensionen reduziert. \n",
    "Der Datensatz wird gezogen aus zwei Gaussverteilungen mit unterschiedlichen Mittelwerten und gleicher Kovarianzmatrix.\n",
    "Die Darstellung unten Zeigt die Punktwolke aus vier Verschiedenen Richtungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=500, n_features=3, random_state=2, cluster_std=3)\n",
    "plots.plot_3d_views(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "transformed = pca.fit_transform(X)\n",
    "\n",
    "plots.plot_3d_views(transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed = pca.fit_transform(X)\n",
    "plt.scatter(transformed[:, 0], transformed[:, 1], c=y, cmap=discrete_cmap)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Gesichtserkennung \n",
    "\n",
    "Man betrachte jeden einzelnen Pixel eines Bildes als Attribut dessen Wert der Grauwert der wert des Attributes ist.\n",
    "\n",
    "So wird ein Bild zu einem 1D Vektor. Mehrere Bilder ergeben wieder die Datenmatrix $\\mathbf{X}$\n",
    "\n",
    "Ein Bild mit $64 \\times 64$ Pixeln wird so zu einem Vektor der Länge $4096$.\n",
    "\n",
    "##### Ein einfacher Gesichtserkennungsalgorithmus\n",
    "\n",
    "Angenommen die Aufgabe wäre die Zuordnung von Fotos aller Studenten der TU zu deren Namen. \n",
    "Gesucht wird also eine Funktion die aus einem Foto einen Namen macht. \n",
    "\n",
    "Idee:\n",
    "1. Speichere Bilder von allen Studenten in einer Matrix $\\mathbf{X}$ der Dimension $\\text{Anzahl Studenten} \\times \\text{Anzahl Pixel}$ und einen Labelvektor $y$ der Länge $N$ der die Namen (oder Matrikelnummern) enthält.\n",
    "2. Berechne die Distanz $D$ zwischen einem neuen Foto $x_{\\text{neu}}$ zu allen in $\\mathbf{X}$ gespeicherten Bildern. \n",
    "3. Gebe zurück das $y_i$ für das $i$ bei dem $D(x_{\\text{neu}}, x_i)$ minimal ist.\n",
    "\n",
    "Probleme: \n",
    " - Alle Bilder zu speichern ist schwierig bis unmöglich. \n",
    " - Die Distanz zu allen Einträgen zu finden kann zu lange dauern.\n",
    " - Wahl des Distanzmaßes in hohen Dimensionen ist nicht trivial.\n",
    " \n",
    "##### Eigenfaces \n",
    "\n",
    "*Original Artikel von 1991 von Turk und Pentland http://www.mitpressjournals.org/doi/10.1162/jocn.1991.3.1.71*\n",
    "\n",
    "Der Input in den Algorithmus ist der selbe wie oben. Die Matrix aller Fotos $\\mathbf{X}$. Diesmal wird diese Matrix jedoch nicht komplett abgespeichert.\n",
    "\n",
    "Idee:\n",
    "1. Wende PCA auf $X$ an. \n",
    "2. Erhalte Transformationsmatrix $\\mathbf{W}$ der Dimension $d \\times k$\n",
    "3. Berechne Gewichte $g_m = \\mathbf{v}_m^T \\cdot(x_i - \\mathbf{\\mu}) $ für jedes gespeicherte Bild $x_i$ und jeden Eigenvektor $\\mathbf{v}_m$ mit $m \\in \\{1, \\ldots, k\\}$ und erhalte so einen Gewichtsvektor $G$ der länge $k$.\n",
    "4. Berechne Distanz $D$ zwischen dem Gewichtsvektor eines neuen Bildes $G_{\\text{neu}}$ zur allen Gewichtsvektoren in der alten Bilder aus. \n",
    "5. Gebe zurück das $y_i$ für das $i$ bei dem $D(G_{\\text{neu}}, G^{i})$ minimal ist.\n",
    "\n",
    "In der Realität ist die Berechnung der PCA auf großen Matrizen nicht immer Trivial.\n",
    "\n",
    "###### Python Beispiel für Eigenfaces\n",
    "\n",
    "Der LFW (Labeled Faces in the Wild) Datensatz ist ein beliebter Datesatz für Algorithmen zur Gesichtserkennung.\n",
    "http://vis-www.cs.umass.edu/lfw/\n",
    "Er enthält etwa 13.000 Bilder von mehreren hundert Personen die aus dem Internet heruntergeladen wurden.\n",
    "\n",
    "Für dieses Beispiel gucken wir uns nur 200 der Bilder an um Rechenzeit zu sparen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.8)\n",
    "\n",
    "n_faces = 200\n",
    "\n",
    "X = lfw_people.data[0:n_faces]\n",
    "y = lfw_people.target[0:n_faces]\n",
    "\n",
    "_, h, w = lfw_people.images.shape\n",
    "img = X[2].reshape(h, w)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Bild Nummer 3 aus $\\mathbf{X}$')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_face = X.mean(axis=0).reshape(h, w)\n",
    "plt.imshow(mean_face, cmap='gray')\n",
    "plt.title('Durschnittliches Gesicht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.imshow(eigenfaces[1], cmap='gray')\n",
    "ax1.set_title('Eigenface zum zweitgrößten Eigenwert')\n",
    "\n",
    "ax2.imshow(eigenfaces[98], cmap='gray')\n",
    "ax2.set_title('Eigenface zum 98ten Eigenwert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \n",
    "\n",
    "Eine anderer Ansatz zur Dimensionsreduzierung. Dabei werden die Daten nicht Transformiert sondern einfach Spalten entfernt die keine/wenig Aussagekraft besitzen. Die Aussagekraft bezieht dabei auf ein zu Lösendes Klassifizierungsproblem. Welche Spalten man verwerfen kann, ist also sehr Problemspezifisch. Im Allgemeinen folgen viele Feature Selection Ansätze der Heuristik:\n",
    "\n",
    "> Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other.\n",
    "\n",
    "> -- Mark Hall\n",
    "\n",
    "### Univariate Feature Selection\n",
    "\n",
    "Betrachtet jedes Attribut für sich alleinstehend. Häufig beinhaltet das auch eine Art der händischen Vorverarbeitung.\n",
    "\n",
    "#### Korrelation mit der Zielgröße\n",
    "\n",
    "Angenommen man wolle eine Größe $y$ aus einem 4D Datensatz schätzen. Ein Teil dieser Daten sind aber mit hohem Rauschen versehen oder haben mit der Zielgröße keinen Kausalen oder Statistischen Zusammenhang.\n",
    "\n",
    "Ein einfacher Algorithmus sucht einfach nach den $k$ Attributen mit denhöchsten Korrelationen und entfernt alle anderen.\n",
    "\n",
    "In dem Beispiel unten wird ein Datensatz erzeugt in welchem nur zwei der 4 vorhandenen Attribute mit der Zielgröße $y$ korrelieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "X, y = make_regression(n_samples=300, n_features=4, n_informative=2, n_targets=1, random_state=0, noise=0.1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "for i in range(4):\n",
    "    ax = axs.flatten()[i] \n",
    "    ax.scatter(X[:, i], y)\n",
    "    ax.set_ylabel('Target Variable')\n",
    "    ax.set_xlabel('X{}'.format(i))\n",
    "    r, _ = pearsonr(X[:, i], y)\n",
    "    print('Korrelation zwischen Attribut {} Ziel: {}'.format(i, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Feature Selection\n",
    "\n",
    "Testet Kombinationen und ganze Untermengen aus Attributen nach verschiedenen Kriterien. Zum Beispiel Korrelation, Mutual Information, Kreuzentropie, Minimal Desciption Length oder einfach die Qualität der Klassifikation/Trennung. \n",
    "\n",
    "\n",
    "Es ist im allgemeinen nicht möglich alle Kombinationen von Attributen zu testen. Die Anzahl der möglichen Kombinationen ist exponentiell. Bei $n$ Attributen müssen nach Binomischen Lehrsatz Kombinationen getestet werden:\n",
    "\n",
    "$$\n",
    "N = \\sum_{k = 1}^{n} \\begin{pmatrix}\n",
    "    n\\\\\n",
    "    k\\\\\n",
    "    \\end{pmatrix} = 2^n -1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Miteinander Korrelierte Attribute \n",
    "\n",
    "In dem Beispiel unten sieht man schnell, dass zwei Attribute miteinander korrelieren. Eines der beiden Attribute ist also überflüssig bzw. redundant. \n",
    "\n",
    "Um miteinander korrelierte Attribute zu finden kann man einfach alle Paare von Attributen miteinander verlgeichen. Der verlgeich von allen Paaren miteinander führt zur quadratischen Laufzeit dieser Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=3, n_informative=2, n_redundant=1, n_repeated=0, n_classes=2, n_clusters_per_class=2, random_state=0)\n",
    "plots.plot_3d_views(X, y)\n",
    "\n",
    "for i, j in combinations([0, 1, 2], 2):\n",
    "    r, p = pearsonr(X[:, i], X[:, j])\n",
    "    print('Korrelation zwischen Attribut {} und {} : {}'.format(i, j, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt Selektionsstrategien und Heuristiken um weniger Kombinitionen zu testen. Die meisten beschränken sich darauf nur Paare von Kombinationen zu testen. \n",
    "Diese art von Heuristik wird deshalb häufig auch als Bivariat bezeichnet.\n",
    "\n",
    "Die beiden einfachsten oder bekanntesten Stretegien sind Forward beziehungsweise Backward Selection\n",
    "\n",
    "\n",
    "###### Forward Selection:\n",
    "\n",
    "Das Verfahren arbeitet Iterativ und testet untermengen nach einem festzulegendem Kriterium.\n",
    "Starte mit dem einzelnen besten Attribut $f_0$ und füge so lange Attribute hinzu bis ein Abbruchkriterium erreicht ist. \n",
    "\n",
    "###### Backward Selection:\n",
    "\n",
    "Wie Forward Selection aber es wird von der vollen Menge an Attributen gestartet und dann iterativ Einträge entfernt.\n",
    "\n",
    "#### Max-Relevance, Min-Redundancy (mRMR)\n",
    "\n",
    "Original veröffentlichung von Peng et al. (2005): [ieeexplore.ieee.org/document/1453511/](ieeexplore.ieee.org/document/1453511/)\n",
    "\n",
    "Wähle die Untermenge an Attributen $S_k = \\{f_1, f_2, \\ldots, f_k\\}$ die Insgesamt die höchste Relevanz bezüglich der Zielvariable $y$ hat und gleichzeitig die Korrelation zwischen den Attributen in $S_k$ möglichst klein ist.\n",
    "\n",
    "Die Relevanz wird zum häufig durch ein Korrelationsmaß oder die sogenannte Mutual Information bestimmt.\n",
    "Mehr zum Thema Mutual Information folgt noch. \n",
    "\n",
    "Für das gesuchte $S_k$ soll gelten $\\max _{S_{k}}(D - R)$ wobei\n",
    "\n",
    "\\begin{align}\n",
    "D(S, y) =& {\\frac {1}{|S|}}\\sum _{f_{i}\\in S}I(f_{i}, y) \\\\\n",
    "R(S)   =& {\\frac {1}{|S|^{2}}}\\sum _{f_{i},f_{j}\\in S}I(f_{i}, f_{j}) \n",
    "\\end{align}\n",
    "\n",
    "Die Untermengen werden gebildet wie bei Forward Selection. Das nächste Attribut wird ausgewählt nach \n",
    "\n",
    "$$\n",
    "\\mathrm {mRMR} =\\max _{S}\\left[{\\frac {1}{|S|}}\\sum _{f_{i}\\in S}I(f_{i}, y)-{\\frac {1}{|S|^{2}}}\\sum _{f_{i},f_{j}\\in S}I(f_{i}, f_{j})\\right].\n",
    "$$\n",
    "\n",
    "Der mRMR Algorithmus gehört zu einer Klasse von Algorithmen die versuchen Relevanz und Redundanz zu minimieren. Er hat einige interessante Eigenschaften aus informationstheoretischer Sicht und ist vor allem in Anwendungen der Biologie und Genetik interessant.\n",
    "\n",
    "\n",
    "Feature Selection ist vor allem dann Wichtig wenn die Anzahl der Attribute größer ist als die Anzahl der Beispiele im Datensatz.\n",
    "\n",
    "\n",
    "### Probleme\n",
    "\n",
    "Algorithmen wie mRMR die sich auf iterative auswahlen verlassen, indem sie zum Beispiel Forward Selection benutzen, werden auch \"greedy\" heuristiken genannt. Es ist nicht immer gewährleistet, dass auch das globale Optimum erreicht wird wenn eine greedy Heuristik benutzt wird.  Das hängt natürlich von der zu optimierenden Zielfunktion ab.\n",
    "\n",
    "Im allgemeinen ist es schwierig bzw. unmöglich in annehmbarer Zeit die \"optimale\" Untermenge an Attributen zu finden.\n",
    "\n",
    "Interresanter Artikel:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multivariate_mutual_information\n",
    "\n",
    "\n",
    "<p style=\"color:gray\"> Für die Theoretiker: Das VERTEX-COVER Problem kann auf MIN_FEATURE reduziert werden. Dadurch wird es NP-Vollständig. http://scottdavies.net/aaai94.pdf </p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (Teil 2)\n",
    "\n",
    "Zur Evaluierung, ob eine Feature Selection tatsächlich *gut* funktioniert,  muss letztendlich das zu lösende Klassifikationsproblem getestet werden. Ist die Klassifikation gut nachdem eine Untermenge der Attribute ausgewählt wurde, so war die Feature Selection erfolgreich. \n",
    "\n",
    "\n",
    "Dazu später mehr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
